# CUMUL - Website Fingerprinting at Internet Scale 分析报告
## 一、论文概况

---



### 标题
Website Fingerprinting at Internet Scale

### 收录会议或期刊
未提及

### 作者
Andriy Panchenko∗, Fabian Lanze∗, Andreas Zinnen†, Martin Henze‡, Jan Pennekamp∗, Klaus Wehrle‡, and Thomas Engel∗

### 摘要
Abstract—The website ﬁngerprinting attack aims to identify the content (i.e., a webpage accessed by a client) of encrypted and anonymized connections by observing patterns of data ﬂows such as packet size and direction. This attack can be performed by a local passive eavesdropper – one of the weakest adversaries in the attacker model of anonymization networks such as Tor. In this paper, we present a novel website ﬁngerprinting attack. Based on a simple and comprehensible idea, our approach outperforms all state-of-the-art methods in terms of classiﬁcation accuracy while being computationally dramatically more efﬁcient。

### 编号
未提及

### 作者邮箱
∗{ﬁrstname.lastname}@uni.lu, †andreas.zinnen@hs-rm.de, ‡{lastname}@comsys.rwth-aachen.de

摘要：本文介绍了一种新的网站指纹识别攻击。该攻击旨在通过观察数据流的模式（如数据包大小和方向）来识别加密和匿名连接的内容（即客户端访问的网页）。本文提出的方法基于一个简单而易于理解的思路，在计算效率方面显著优于所有现有的方法，同时在分类精度方面也具有更好的表现。该攻击可以由本地被动窃听者执行，而本文的方法可用于互联网规模的攻击。此外，作者提供了一些对抗此种攻击的建议。

---



## 二、论文翻译



## 

---

 ## 原文[0/19]： 

 

 Andriy Panchenko∗, Fabian Lanze∗, Andreas Zinnen†, Martin Henze‡, Jan Pennekamp∗, Klaus Wehrle‡, and Thomas Engel∗ 

∗University of Luxembourg (LU), †RheinMain University of Applied Sciences (DE), ‡RWTH Aachen University (DE) E-mail: ∗{ﬁrstname.lastname}@uni.lu, †andreas.zinnen@hs-rm.de, ‡{lastname}@comsys.rwth-aachen.de I. Anonymous communication on the Internet is about hiding the relationship between communicating parties. For many people, in particular for those living in oppressive regimes, the use of anonymization techniques is the only way to exercise their right to freedom of expression and to freely access information, without fearing the consequences. Besides, these techniques are often used to bypass country-level censorship. Hence, the users of anonymization techniques strongly rely on the underlying protection as deﬁned in their attacker model. For them, it is particularly important to know the level of protection provided against considered adversaries. Several methods for low-latency anonymous communication had been proposed by the research community but only a few systems have been actually deployed. The Tor network [8] – the most popular system nowadays that is used by millions of daily users – promises to hide the relationship between the sender of a message and its destination from a local observer. This is the entity that eavesdrops trafﬁc between the sender and the ﬁrst anonymization node. It can be, for example, a local system administrator, an ISP, or everyone in the sending range of a signal if the user is connected via a wireless link. An entity with such capabilities is one of the weakest adversaries in the attacker model of this and other anonymization techniques [8]. The website ﬁngerprinting (WFP) attack is a special case of trafﬁc analysis. Performed by a local eavesdropper, it aims to infer information about the content (i.e., the website visited) of encrypted and anonymized connections by observing patterns of data ﬂows. Here, the attacker merely utilizes meta infor mation, such as packet size and direction of trafﬁc, without breaking the encryption. Before the end of 2011, Tor was con sidered to be secure against this threat [11], [21]. Since then, it has become an active ﬁeld of research. Several related works showed the feasibility of the WFP attack in Tor, however, using relatively small datasets (compared to the size of the world wide web) and proposed voluminous countermeasures. A recent work [14] questions the practical realizability of the attack in the light of assumptions typically made and the impact of the base-rate fallacy on the classiﬁcation results. In this paper, we propose a novel WFP attack based on a subtle method to map network traces into a robust representation of a class (in machine learning, a class is a set or collection of abstracted objects that share a common characteristic; in our case these are traces recorded for the same webpage). We abstract the loading process of a webpage by generating a cumulative behavioral representation of its trace. From this, we extract features for our classiﬁer. These implicitly cover characteristics of the trafﬁc that other classi ﬁers have to explicitly consider, e.g., packet ordering or burst behavior. By design, our classiﬁer is robust against differences in bandwidth, congestion, and the timing of a webpage load. As we will show, this approach outperforms all existing state of-the-art classiﬁers in terms of classiﬁcation accuracy while being computationally tremendously more efﬁcient. To evaluate the severity of the WFP attack in reality, we constructed the most representative dataset that has ever been assembled in this domain. It consists of over 300,000 webpages (this is ten times larger than the biggest set used before, i.e., the one described in [14]) and is not subject to simpliﬁed assumptions made by the related work. For instance, most researchers consider only the index pages, i.e., those that web servers provide for a requested domain (e.g., [26], [9], [11]). Their objective is limited to differentiating index pages, i.e., to detect certain index pages within a set of other index pages. We clearly differentiate the classiﬁcation of webpages and websites. Our datasets enable for the ﬁrst time to study the detectability of both single webpages and complete websites within realistic Internet trafﬁc (serving as background noise). Further, we do not limit our dataset to the most popular websites according to Alexa1, since we argue that their index page cannot serve as realistic background trafﬁc. For our datasets, we combined different sources for information such as links distributed via Twitter or traces of a Tor exit node in order to create a random and representative sample of webpages actually visited on the Internet (or, overTor in particular) at the time of evaluation.

 We use our superior attack together with our collected data to study the limits of webpage and website ﬁngerprinting at Internet scale. We investigate the probability of success for different strategies a realistic adversary may follow. We show that with existing classiﬁers under realistic conditions, webpage ﬁngerprinting for any webpage is similar to ﬁnding a needle in a haystack – in general it is doomed to failure. However, website ﬁngerprinting, despite being a more realistic scenario, is also easier to handle for existing classiﬁers. Our evaluation reveals several tactics that increase the probabilityfor a successful attack.  

---

 ## 翻译[0/19]：
 

Andriy Panchenko∗，Fabian Lanze∗，Andreas Zinnen†，Martin Henze‡，Jan Pennekamp∗，Klaus Wehrle‡和Thomas Engel∗

∗卢森堡大学（LU），†莱茵河应用科学大学（DE），‡RWTH Aachen大学（DE）电子邮件：∗{ﬁrstname.lastname} @uni.lu，†andreas.zinnen@hs-rm.de，‡{lastname}@comsys.rwth-aachen.de

I.互联网上的匿名通信是关于隐藏通信方之间的关系。对于许多人，特别是生活在压迫政权下的人来说，使用匿名化技术是行使他们的言论自由权和自由获取信息的唯一途径，而不用担心后果。此外，这些技术常常被用来绕过国家层面上的审查。因此，匿名化技术的用户强烈依赖于他们攻击者模型中所定义的潜在保护。对于他们而言，了解所考虑的对手所提供的保护水平尤为重要。研究界已提出了几种低延迟匿名通信的方法，但只有少数系统实际上被部署。 Tor 网络[8] - 目前最受欢迎的系统，每天有数百万用户使用 - 承诺从本地观察者隐藏消息发送者和其目的地之间的关系。这是窃听发送者和第一个匿名化节点之间的流量的实体。例如，它可以是本地系统管理员、ISP，或者如果用户通过无线链路连接，则是发送范围内的任何人。具有这种能力的实体是此及其他匿名化技术的攻击者模型中最弱的对手之一[8]。网站指纹攻击（WFP）是流量分析的一个特殊案例。由本地窃听者执行，其目的是通过观察数据流模式推断加密和匿名连接内容（即访问的网站）。在这里，攻击者仅利用元信息，如数据包大小和流量方向，而不破解加密。在2011年底之前，Tor 被认为是安全的，能够抵御这种威胁[11]，[21]。自那时以来，它已成为研究的一个活跃领域。几项相关的工作显示了 WFP 攻击在 Tor 中的可行性，但使用的数据集相对较小（与全球网络的规模相比），并提出了大量的对抗措施。一项最近的工作[14]在通常假设和基本率谬误对分类结果的影响下，质疑该攻击的实际可实现性。在本文中，我们提出了一种基于微妙的方法将网络跟踪映射到一个类的强大表示的 WFP 攻击。在机器学习中，类是一组或多个具有共同特征的抽象对象集合，在我们的案例中，这些是为同一网页记录的跟踪。我们通过生成网页加载过程的累积行为表示来抽象网页加载过程。从此，我们提取我们分类器的特征。这些隐含地涵盖了其他分类器必须明确考虑的流量特征，例如包排序或突发行为。按设计，我们的分类器对带宽、拥塞和网页加载时机的差异具有鲁棒性。正如我们将展示的那样，与所有现有的最先进分类器相比，这种方法在分类准确性方面表现更好，同时在计算上更加高效。为了评估 WFP 攻击在现实中的严重性，我们创建了迄今为止在该领域中组装的最具代表性的数据集。它由超过300,000个网页组成（这比先前使用的最大集合，即[14]中描述的集合大十倍），并且不受相关工作所做的简化假设的影响。例如，大多数研究人员仅考虑索引页面，即Web服务器为请求的域名（例如[26]，[9]，[11]）提供的页面。他们的目标仅限于区分索引页面，即在其他索引页面集中检测特定的索引页面。我们清楚地区分网页和网站的分类。我们的数据集首次使得在现实互联网流量中研究单个网页和完整网站的可检测性成为可能（作为背景噪声）。此外，我们不限制我们的数据集为 Alexa1 最流行的网站，因为我们认为它们的索引页面不能作为现实的背景流量。为了创建我们的数据集，我们结合了不同的信息来源，例如通过 Twitter 分发的链接或 Tor 出口节点的跟踪，以创建实际上在评估时访问的互联网上的随机和代表性的网页样本（或特别是 overTor）。

我们使用我们卓越的攻击和搜集的数据来研究互联网规模下网页和网站指纹识别的限制。我们调查了现实敌手可能采用的不同策略的成功概率。我们发现，在现实情况下，使用现有的分类器进行网页指纹识别就像在干草堆中找针一样 - 通常注定失败。然而，网站指纹识别，尽管是一个更现实的场景，但对于现有的分类器来说也更容易处理。我们的评估揭示了几种增加攻击成功概率的策略。

## 

---

 ## 原文[1/19]： 

 
 The contributions of this paper are as follows: 1) We propose a novel WFP attack on Tor based on the idea to sample features from a cumulative rep resentation of a trace. We show that our approach outperforms all attacks existing in the literature on the state-of-the-art dataset in terms of classiﬁcation accuracy while being computationally more efﬁcientby orders of magnitude.

 2) We provide the most comprehensive dataset to evalu ate the WFP attack. Instead of being limited to index pages of popular websites, it contains various web pages actually retrieved on the Internet. We managed to assemble more than 300,000 of such pages. 3) Even allowing the attacker an optimal strategy, we show that webpage ﬁngerprinting at Internet scale is practically unfeasible on the one hand while website ﬁngerprinting has a chance to succeed on the other. We explore neglected aspects of the attack and inves tigate the realistic probability of success for different strategies a real-world attacker may follow.II.

 Tor (The Onion Router) is the most popular anonymization network to date with more than two million daily users2. It is designed particularly for low-latency applications such as web browsing. Tor routes connections through virtual tun nels, called circuits, which typically consist of three onion routers3 (OR). The trafﬁc is encrypted in layers, i.e., the client establishes a symmetric encryption key with each OR on the circuit, encrypts the data with all keys consecutively, and each OR decrypts its layer on the path. This technique ensures that no relay on a path can know both the origin and destination of a transmission at the same time. The goal of Tor is to improve users’ privacy by hiding routing information and communication content. However, Tor is not able to obscure the size, direction and timing of transmitted packets. Information leakage based on these metrics constitutes the foundation of the website ﬁngerprinting attack. The objective is to match patterns of a website load trace to a previously recorded trace in order to reveal which particular website a user is visiting over the anonymized and encrypted path. Typically, multiple traces of a single website are retrieved and analyzed.These are called instances.

 Website ﬁngerprinting is commonly evaluated in two sce narios: in the closed-world scenario, the number of websites a user may visit is limited to a ﬁxed number. Obviously, this scenario is not realistic. However, it is suitable to compare and analyze the performance of classiﬁcation approaches. In the more realistic open-world scenario, the adversary tries to identify whether a visited website belongs to a given set of monitored websites even though the user may also visit sites unknown to the adversary. Here, we call this set of sites, which are unknown, the background set and the set of monitored sites the foreground set, correspondingly. In the remainder of this paper we clearly distinguish between the terms “website” and “web page”. A website is a collection of web pages, which are typically served from a single web domain. The initial web page of a website is called the index page4. This page is served by the web server when a user queries the domain name of the corresponding website. In the related work, website ﬁngerprinting is commonly applied only for such index pages. In our evaluation, we extend the universe to arbitrary web pages, and differentiate between the objectives of an adversary, e.g., to monitor all pages belonging to a particular website, orto monitor a single particular web page.  

---

 ## 翻译[1/19]：
 

本文的贡献包括：1）我们提出了一种新的基于采样特征的WFP攻击Tor的方法，该方法基于跟踪的累积表示，我们证明了我们的方法在分类准确性方面优于现有文献中的所有攻击，同时计算效率比现有方法高出数个数量级。2）我们提供了最全面的数据集来评估WFP攻击。与仅限于流行网站的索引页不同，它包含实际从互联网检索的各种网页。我们设法收集了超过300,000个此类网页。 3）即使允许攻击者采用最优策略，我们也证明互联网规模的网页指纹识别在一方面是实际上不可行的，而网站指纹识别在另一方面有成功的机会。我们探讨了攻击的被忽视的方面，并研究了现实中攻击者可能采用的不同策略的成功概率。

Tor（洋葱路由器）是迄今为止最受欢迎的匿名化网络，拥有超过两百万的每日用户，它专门为低延迟应用程序（如web浏览）设计。Tor通过虚拟隧道（称为电路）路由连接，通常由三个洋葱路由器（OR）组成。流量被分层加密，即客户端与电路上的每个OR建立对称加密密钥，按顺序将数据以所有密钥加密，并且每个OR解密其所在的路径层。这种技术确保了路径上的中转不会同时知道传输的源和目标。Tor的目标是通过隐藏路由信息和通信内容来提高用户的隐私。然而，Tor不能掩盖传输数据包的大小、方向和时间。基于这些指标的信息泄漏构成了网站指纹识别攻击的基础。目标是将网站加载跟踪的模式与先前记录的跟踪匹配，以揭示用户在匿名和加密路径上访问的特定网站。通常，检索和分析单个网站的多个痕迹，这些痕迹称为示例。

网站指纹识别通常在两种场景下进行评估：在封闭世界的场景中，用户可能访问的网站数量受到限制。显然，这种情况不现实。然而，它适合比较和分析分类方法的性能。在更现实的开放世界场景中，攻击方试图确定访问的网站是否属于给定的一组受监控的网站，即使用户还可能访问攻击者不知道的站点。在这里，我们将该组未知网站称为背景集，被监控网站的集合称为前景集，对应分别。在本文的其余部分，我们明确区分“网站”和“网页”这两个术语。网站是Web页面的集合，通常从单个Web域提供服务。网站的初始Web页面称为索引页面。当用户查询相应网站的域名时，该页面由Web服务器提供。在相关工作中，网站指纹识别通常仅应用于此类索引页面。在我们的评估中，我们将宇宙扩展到任意网页，并区分攻击者的目标，例如，监视属于特定网站的所有页面，或监视单个特定网页。

## 

---

 ## 原文[2/19]： 

 
 Attacker Model We assume the attacker to be a passive observer. He does not modify transmissions and he is not able to decrypt packets. The attacker is able to monitor trafﬁc between the user and the entry node of the Tor circuit. Hence, he either monitors the link itself or a compromised entry node. Further, we assume the attacker to possess sufﬁcient computational resources to train the ﬁngerprinting technique on large training datasets.III.

 As early as 1996, Wagner and Schneier discovered that trafﬁc analysis can be used to draw conclusions about the content of encrypted SSL packets [24]. We categorize the related work in this research domain into trafﬁc analysis on encrypted connections in general, website ﬁngerprinting on anonymization networks in particular, and countermeasures that have been proposed against such attacks. A. Trafﬁc Analysis on Encrypted Connections The ﬁrst implementation of a website ﬁngerprinting attack was described by Cheng and Avnur [7] in 1998. By looking at ﬁle sizes, the authors aimed to identify which speciﬁc ﬁle was accessed on a known server over an SSL-protected connection. Similarly, Hintz [12] targeted identifying individual websites when the server is not known, e.g., when using an anonymiza tion proxy. In order to detect whether a website from a given blacklist had been visited over an SSL-protected connection, 2 Sun et al. [23] proposed Jaccard’s coefﬁcient as a metric for the similarity between observed and pre-collected trafﬁc patterns, allowing websites with slightly varying sizes to be matched. These early works showed the general feasibility of the website ﬁngerprinting attack by considering the total sizes of resources. However, they assumed that each request is associated with a separate TCP connection – a constraint that only holds for early versions of the HTTP protocol. Nowadays, HTTP makes use of persistent connections and pipelining5 to improve performance. Hence, it is no longer possible to trivially distinguish between single web object requests. Bissias et al. [3] were the ﬁrst to perform website ﬁngerprinting based on IP packet sizes and inter-packet arrival times instead of web object sizes. This allows the attack to be generalized to VPN or SSH tunnels as well as WPA-protected wireless networks. To further improve the attack, Liberatore and Levine [16] compared the effectiveness of Jaccard’s coefﬁcient and the na¨ıve Bayes classiﬁer on SSH-protected channels. Lu et al. [18] showed that website ﬁngerprinting can be improved by considering information about packet ordering. Several related works do not focus on website ﬁngerprinting in particular, but rather on the detection of other distinct characteristics of network trafﬁc, e.g., the language of a Voice-over-IP (VoIP) call [30], or spoken phrases in encrypted VoIP calls [29]. Gong et al. [10] even showed the feasibility of a remote trafﬁc analysis (where the adversary does not directly observe the trafﬁc pattern) byexploiting queuing side channel in routers.

 B. WFP in Anonymization Networks In 2009, Herrmann et al. [11] were the ﬁrst to apply website ﬁngerprinting to the anonymization networks JAP [2] and Tor [8] as well as on OpenSSH, OpenVPN, Stunnel, and Cisco IPsec-VPN. In addition to the classiﬁers used by Liberatore and Levine, the authors also evaluated a multinominal na¨ıve Bayes classiﬁer. Using this classiﬁer and a dataset consisting of 775 index pages, they achieved recognition rates above 90% for single-hop systems, but only 20% for JAP, and as low as 2.95% for Tor. Therefore, Tor was considered to be secure against website ﬁngerprinting until Panchenko et al. [21] increased the recognition rate for Tor to an alarming degree using an approach based on Support Vector Machines (SVM) in 2011: in the dataset provided by Herrman et al., they recognized more than 54% of the URLs correctly when accessed over Tor. Moreover, the authors were the ﬁrst to evaluate website ﬁngerprinting in an open-world scenario, i.e., they recognized a small number of (monitored) pages in a set of thousands of unknown, random pages that classiﬁer has never seen before. Here, they achieved a recognition rate of up to 73%. These results spawned a signiﬁcant amount of interestin the research community.  

---

 ## 翻译[2/19]：
 

攻击者模型

我们假设攻击者是一名被动观察者。他不会修改传输，也无法解密数据包。攻击者能够监视用户和Tor电路入口节点之间的流量。因此，他要么监视链接本身，要么监视受到攻击的入口节点。此外，我们假设攻击者拥有足够的计算资源来训练大型训练数据集上的指纹识别技术。

1996年，Wagner和Schneier发现流量分析可以用于推断加密SSL数据包的内容[24]。我们将该研究领域相关工作分为一般加密连接的流量分析、特定于匿名网络的网站指纹识别以及针对此类攻击提出的对策。

A. 加密连接的流量分析

1998年，Cheng和Avnur首次实现了网站指纹识别攻击 [7]。通过查看文件大小，作者旨在识别通过SSL保护的已知服务器上访问的特定文件。同样，Hintz [12] 旨在在服务器未知时识别单个网站，例如使用匿名代理时。为了检测是否已经通过SSL保护的连接访问了给定黑名单中的网站，Sun等人[23] 提出了Jaccard系数作为观察到的流量模式与预先收集的流量模式之间的相似度计量标准，允许匹配稍有不同大小的网站。这些早期工作通过考虑资源的总大小证明了网站指纹识别攻击的普遍可行性。然而，他们假设每个请求与单独的TCP连接相关联，这是HTTP协议早期版本才存在的限制。现在，HTTP使用持久连接和流水线技术[5]来提高性能。因此，不再可能轻松地区分单个网页对象请求。

Bissias等人[3]首先基于IP数据包大小和数据包间隔时间而不是网页对象大小进行了网站指纹识别。这使得攻击也可推广到虚拟专用网络（VPN）或SSH隧道以及WPA保护的无线网络。为了进一步改进攻击，Liberatore和Levine[16]比较了基于SSH保护通道上Jaccard系数和朴素贝叶斯分类器的有效性。Lu等人[18]表明，通过考虑数据包排序信息，可以改进网站指纹识别攻击。还有一些相关工作不特别针对网站指纹识别，而是关注网络流量的其他明显特征的检测，例如Voice-over-IP（VoIP）通话的语言[30]或加密VoIP通话中的口语短语 [29]。 Gong等人[10]甚至证明了远程流量分析的可行性（其中攻击者不直接观察流量模式）通过利用路由器中的排队侧信道。

B. 匿名化网络中的网站指纹识别

在2009年，Herrmann等人[11]首先将网站指纹识别应用于匿名化网络JAP [2]和Tor [8]，以及OpenSSH，OpenVPN，Stunnel和Cisco IPsec- VPN。除了Liberatore和Levine使用的分类器外，作者还评估了一个多项式朴素贝叶斯分类器。使用这个分类器和一个包含775个索引页的数据集，他们对单跳系统实现了高于90％的识别率，但对于JAP仅有20％，对于Tor则低至2.95％。因此，直到2011年Panchenko等人 [21]基于支持向量机（SVM）的方法将Tor的识别率提高到了令人担忧的程度，Tor被认为是安全的：在Herrman等人提供的数据集中，他们正确识别超过54％的URL。此外，该作者是第一个在开放世界场景下评估网站指纹识别攻击的人，即他们在成千上万个未知的随机网页中识别一小部分（已监视的）页面，分类器从未见过这些页面。在这里，他们实现了高达73％的识别率。这些结果在研究界引起了相当大的关注。

## 

---

 ## 原文[3/19]： 

 
 Dyer et al. [9] compared existing classiﬁers and additional features on datasets with 2, 128, and 775 websites. However, their proposed time, bandwidth, and variable n-gram classiﬁers did not improve the recognition rate compared to the approach of Panchenko et al. in any of the considered scenarios. In 2012, Cai et al. [5] presented an approach achieving a recognition rate of over 80% for a dataset with 100 URLs and over 70% for 800 URLs. Like Panchenko et al., they utilized an SVM, but their features are based on the optimal string alignment distance (OSAD) of communication traces. They were the ﬁrst to study the recognition of different pages of a website and the effect of clicking on embedded links, i.e., browsing within the same website, using a Hidden Markov Model. Though such a user behavior turned out to be detectable with a high probability, their study was limited to two websites only. Wang et al. [26] improved the optimal string alignment distance approach of Cai et al. and enhanced the data preparation methods by statistically removing Tor management packets. With these improvements they obtained recognition rates of better than than 90% for both the closed-world (100 URLs) and the open-world (1,000 URLs) scenarios. Recently, the authors further improved the recognition rates in larger open-world scenarios (> 5,000 URLs) using a novel k-Nearest Neighbor (k-NN) classiﬁer, which also signiﬁcantly reduces the time necessary for training compared to previous results [25]. The latest contributions to the ﬁeld of website ﬁnger printing in Tor were made by Juarez [14], Cai [4], and Kwon et al. [15]. Juarez et al. [14] critically evaluate typical assumptions in WFP attacks. They showed that the accuracy of classiﬁcation decreases by 40% in less than 10 days and further declines almost to zero after 90 days for Alexa Top 100 pages due to content change. Also, the accuracy drops dramatically if a user performs multitab browsing or if different generations of the Tor Browser Bundle (TBB) are used for training and testing. Unfortunately, their analysis did not con sider an attacker that is able to use different versions/settings for training although a realistic adversary would have this capability. Moreover, the authors observe a similar impact on the attack, if the adversary is not able to train using exactly the same Internet connection as the user. The authors are the ﬁrst to consider the base-rate fallacy in the scope of the WFP attack6. They show that, though the accuracy of classiﬁcation is very high, due to a large universe size, in most of the cases the adversary would wrongly conclude that the user had accessed a monitored page. To improve the situation, they propose to refrain from positive classiﬁcation if the probability difference between the two closest classiﬁcation decisions is below acertain threshold.

 Cai et al. [4] analyze WFP attacks and defenses from a theoretical perspective using a feature-based comparative methodology. The goal is to provide bounds on the effec tiveness of proposed defenses, i.e., to which extent certain defenses hide which feature. Moreover, the authors propose a methodology to transfer closed-world results to an open-world setting. However, we argue that their theoretical deﬁnition of the corresponding open-world classiﬁer cannot hold in practice. The key idea in the open-world scenario is to test a classiﬁer on traces of websites it has never seen before. In practice, it cannot be guaranteed that an open-world classiﬁer identiﬁes a monitored page if and only if the corresponding closed-world classiﬁer detects that particular page as deﬁned in their work. Instead of deriving theoretical bounds, we performa practical evaluation of the attack.

 Recently, Kwon et al. [15] have applied the WFP attack in the scope of Tor hidden services. Their approach can 3 substantially distinguish a Tor hidden service connection from a regular Tor circuit – assuming the attacker controls the entry node – but has only moderate success in differentiatingbetween hidden services.

 C. Countermeasures against WFP Several countermeasures have been proposed to protect against website ﬁngerprinting attacks. Padding as a basic coun termeasure was ﬁrst studied by Liberatore and Levine [16]. Tor employs padding to generate cells of a ﬁxed size, which are indistinguishable. While padding operates on a per-packet level, trafﬁc morphing aims to adapt a complete packet trace such that it looks similar to another packet trace [31]. However, Dyer et al. [9] showed trafﬁc morphing to be ineffective as adefense against WFP in practice.  

---

 ## 翻译[3/19]：
 

Dyer等人[9]比较了现有分类器和不同特征在包含2、128和775个网站的数据集上的表现。然而，他们提出的时间、带宽和变量n-gram分类器在任何情况下都没有比Panchenko等人的方法提高识别率。2012年，Cai等人[5]提出了一种方法，可以对具有100个URL的数据集进行超过80%的识别率和800个URL的70%以上的识别率。像Panchenko等人一样，他们利用了SVM，但是他们的特征基于通讯跟踪的最佳字符串对齐距离（OSAD）。他们是第一批研究网站的不同页面识别和点击嵌入链接（即在同一网站内浏览）对识别的影响的人，使用了一种隐马尔可夫模型。尽管这种用户行为的概率很高，但他们的研究仅限于两个网站。Wang等人[26]改进了Cai等人的最佳字符串对齐距离方法，并通过统计地删除Tor管理数据包来改进数据准备方法。通过这些改进，他们在封闭世界（100个URL）和开放世界（1,000个URL）情况下均获得了超过90%的识别率。最近，作者使用一种新颖的k最近邻(k-NN)分类器在更大的开放世界场景（> 5,000个URL）中进一步提高了识别率，这也显著减少了培训所需的时间，与以前的结果相比[25]。Juarez[14]、Cai[4]和Kwon等人[15]在Tor的网站指纹识别领域做出了最新的贡献。Juarez等人[14]批评了WFP攻击的典型假设。由于内容更改，他们表明在不到10天内分类的准确度降低了40％，并在90天后进一步下降到几乎为零，对于Alexa前100个页面。此外，如果用户执行多选项卡浏览或使用不同版本的Tor Browser Bundle(TBB)进行培训和测试，则准确性会急剧下降。不幸的是，他们的分析没有考虑到能够使用不同版本/设置进行训练的攻击者，尽管现实中的敌手会具备这种能力。此外，如果对手不能使用与用户完全相同的互联网连接进行训练，则攻击的影响也会相似。作者是第一批在WFP攻击的范围内考虑基本速率谬论的人。他们表明，尽管分类的准确性非常高，但由于宇宙大小很大，在大多数情况下，攻击者会错误地得出用户已访问受监控页面的结论。为了改善情况，他们建议如果两个最接近的分类决策之间的概率差异低于某个阈值，则不要进行正分类。Cai等人[4]采用基于特征的比较方法从理论角度分析了WFP攻击和防御。目标是提供关于所提出的防御措施的有效性的界限，即到达哪个程度某些防御措施隐藏哪些特征。此外，作者提出了一种将封闭世界结果转移到开放世界环境中的方法。然而，我们认为他们所提供的理论定义相应的开放世界分类器在实践中无法成立。在开放世界场景中的关键想法是在没有见过的网站迹象上测试分类器。事实上，无法保证开放世界分类器确定监视页是否为定义为其工作的相应封闭世界分类器检测到该特定页。我们没有推导出理论界限，而是对攻击进行了实际评估。最近，Kwon等人[15]已经将WFP攻击应用于Tor隐藏服务的范围内。他们的方法可以在攻击者控制入口节点的情况下显着区分Tor隐藏服务连接和常规Tor电路，但在区分隐藏服务方面成功率只有中等水平。有几种针对网站指纹识别攻击的反制措施被提出。填充作为一种基本的反制措施首先由Liberatore和Levine[16]研究。Tor采用填充来生成大小相同且难以区分的单元格。虽然填充是在每个数据包级别上操作，但流量变形旨在适应完整的数据包跟踪，使其看起来类似于另一个数据包跟踪[31]。然而，Dyer等人[9]表明流量变形在实践中作为WFP的防御是无效的。

## 

---

 ## 原文[4/19]： 

 
 A number of countermeasures aim to create a continuous data ﬂow. Panchenko et al. [21] proposed creating background noise by loading a random website in parallel with the actually desired website thus obfuscating the real transmission. How ever, Wang et al. [25] stated that this approach is not powerful enough to prevent website ﬁngerprinting if the trafﬁc overhead is to be kept reasonable. Introducing BuFLO (Buffered Fixed Length Obfuscation), Dyer et al. [9] reduced the amount of information exploitable by an adversary, by sending packets with a ﬁxed size and at ﬁxed intervals. Cai et al. pointed out several disadvantages of this approach [5]: besides a high overhead in bandwidth and time, BuFLO may reveal the total transmission size under certain conditions and further is not able to adapt for congestion. To overcome these ﬂaws, they proposed Congestion-Sensitive BuFLO (CS-BuFLO). Cai et al. also proposed Tamaraw [4], a heavily-modiﬁed version of BuFLO, which improves performance primarily by treating incoming and outgoing packets differently. Glove [20] is an SSH-based defense that uses knowledge of website traces for trafﬁc morphing. The idea is to cluster all web pages into large similarity groups and add only a small amount of cover trafﬁc to make all the pages within a cluster indistinguishable. Hence, the attacker can only identify the cluster to which the web page belongs, but not the web page itself. Built upon Tamaraw, a similar idea – called Supersequence – is proposed by Wang et al. [25]. However, to be successful, this approach needs to have a-priori information about each page to be protected. To overcome this, Wang and Goldberg propose Walkie Talkie [28] – a general defense that enables the Tor Browser to transmit in half-duplex mode. The idea is to buffer packets in one direction and send them in bursts together with dummy trafﬁc. This usually results in a lower bandwidth overhead compared to Tamaraw or Supersequence and allows for a variable packetrate to deal with congestion.

 Finally, several countermeasures at the application layer have been proposed that do not introduce additional trafﬁc. As a response to the evaluation of Panchenko et al. [21], the Tor project released an experimental patch of the Tor Browser Bundle, which randomizes the pipeline size (i.e., the quantity of requests processed in parallel) and the order of requests for embedded website objects. This technique is called randomized pipelining [22]. HTTPOS [19] (HTTP Obfuscation) follows a similar approach of altering packet sizes, web object sizes, and timing by modifying HTTP and TCP requests. This is achieved, e.g., by changing the HTTP accepted-range header ﬁeld that is used to specify the byte range of a requested resource. This functionality is typically utilized to resume a download of a larger web object. The client can, for example, change the trafﬁc pattern of requesting a large resource to the trafﬁc pattern of multiple requests of small resources. However, both Cai et al. [5] and Wang et Goldberg [26] showed that these defenses are not as effective as assumed and, in the case of randomized pipelining, might even lead to increased recognition rates on small datasets. Since the severity of the WFP attack has not been comprehensively studied to date, none of these countermeasures is currently applied in Tor.IV.

 This section describes the datasets used in our evaluation. The signiﬁcance and plausibility of results strongly depends on the dataset used for training and testing a classiﬁer. In general, a dataset should be a representative, independent, random sample of the considered universe, here, the world wide web. All prior works in this area limited their samples to index pages of the most popular websites or small sets of sites known to be blocked in certain countries. We argue that these datasets do not reﬂect a representative sample of the Internet. First, they do not contain any webpage of a site besides the index page while the majority of retrieved web pages are not main pages but articles, proﬁles or any other type of sub-page. Users surﬁng the web often follow links, e.g., communicated through social networks or they retrieve the index page of a website in order to manually select interesting links. This is particularly important when conducting an open-world analysis. Even if the attack focuses on ﬁngerprinting certain index pages, it is important to use realistic trafﬁc traces as background data to evaluate the success. This fact has been neglected in the related research, i.e., the problem has been simpliﬁed to classify index pages within a set of other index pages instead of real trafﬁc. Second, none of the existing datasets allows an evaluation of ﬁngerprinting for complete websites, though this constitutes an attack scenario to be expected in reality (a censor may rather be interested in blocking or monitoring access to Facebook entirely instead of blocking the Facebook login-page only). Third, the world wide web consists of billions of pages. Therefore, results obtained on small datasets do not allow generalization. Our novel datasets aim to avoid the limitations described above. Additionally, we tested our methods also on data provided by other researchers, particularly to compare the effectiveness of our attack. We now describe the compilationof our datasets in detail.  

---

 ## 翻译[4/19]：
 

一些对策旨在创建连续的数据流。Panchenko等人[21]建议加载一个随机网站与实际所需网站并行以模糊化真实传输，从而创建背景噪音。然而，Wang等人[25]指出，如果保持流量开销合理，这种方法不足以防止网站指纹识别。Dyer等人通过引入BuFLO(缓冲固定长度混淆)，减少对手可利用的信息量，通过发送固定大小和固定间隔的数据包。Cai等人指出此方法存在一些缺点[5]：除带宽和时间的高开销外，BuFLO在某些情况下可能会透露传输的总大小，并且不能适应拥塞。为了克服这些缺陷，他们提出了拥塞敏感的BuFLO（CS-BuFLO）。Cai等人还提出了Tamaraw[4]，这是一种经过大量修改的BuFLO版本，通过将传入和传出的数据包进行不同处理来改进性能。Glove[20]是一种基于SSH的防御，使用网站跟踪的知识进行流量变形。其想法是将所有网页分组为大型相似群体，仅添加少量覆盖流量，使所有属于同一群组的页面不可区分。因此，攻击者只能识别网页所属的群组，而不是网页本身。建立在Tamaraw之上，Wang等人[25]提出了一种类似的想法，称为Supersequence，但为取得成功，此方法需要预先知道要保护的每个页面。为了克服这个问题，Wang和Goldberg提出了Walkie Talkie[28] - 一种使Tor浏览器能够以半双工模式传输的通用语言。其想法是将包缓冲在一个方向中，并与虚假流量一起突发发送。与Tamaraw或Supersequence相比，这通常导致更低的带宽开销，并允许变量数据包速率以处理拥塞问题。

最后，已提出了几种应用层的对策，不会引入额外的流量。作为对Panchenko等人[21]评估的一种响应，Tor项目发布了Tor浏览器捆绑包的实验性补丁，该补丁使管道大小（即并行处理的请求数量）和嵌入式网站对象的请求顺序发生随机变化。这种技术称为随机流水线[22]。HTTPOS[19]（HTTP混淆）采用类似的方法，通过修改HTTP和TCP请求来改变数据包大小，网页对象大小和时间。例如，通过更改HTTP接受范围标头字段来指定所请求资源的字节范围来实现这一点。这通常用于恢复更大网页对象的下载。客户端可以将请求大资源的流量模式更改为多个请求小资源的流量模式。然而，Cai等人[5]和Wang等人[26]表明，这些防御措施并不像假定的那样有效，在随机流水线的情况下，可能会导致小数据集上的更高识别率。由于WFP攻击的严重性目前还未得到全面研究，因此当前在Tor中没有应用这些对策。

本节介绍了我们评估中使用的数据集。结果的显著性和可信度强烈依赖于用于训练和测试分类器的数据集。一般来说，数据集应该是所考虑宇宙（在这里是互联网）的代表性、独立随机样本。在这个领域的所有先前工作都将样本限制在最受欢迎的网站主页或已知在某些国家被屏蔽的小型网站集。我们认为这些数据集不能反映互联网的代表性样本。首先，它们不包含除主页以外的任何网站页面，而大多数检索到的网页不是主页，而是文章、个人资料或任何其他类型的子页面。用户浏览网页时经常按照链接进行导航，例如，通过社交网络进行通信或获取网站的主页以手动选择感兴趣的链接。在进行开放式分析时，这一点尤为重要。即使攻击专注于在一组其他主页中对主页进行指纹识别，使用实际的流量跟踪数据来评估成功仍然很重要。这一点在相关研究中已经被忽略，即问题已经简化为在一组其他主页中对主页进行分类，而不是使用真实流量数据。其次，现有的数据集都不允许评估完整网站的指纹识别，尽管这构成了在现实中预期的攻击场景（审查员可能更愿意完全阻止或监测对Facebook的访问，而不仅仅是阻止Facebook的登录页面）。第三，全球网页的数量可能在数十亿级别，因此使用小型数据集得到的结果不能推广。我们的新型数据集旨在避免上述限制。此外，我们还在其他研究人员提供的数据上测试了我们的方法，特别是为了比较我们攻击的有效性。现在，我们详细描述我们的数据集编制。

## 

---

 ## 原文[5/19]： 

 
 A. Data sets provided by Wang et al. To compare the performance of different approaches in terms of classiﬁcation accuracy and computation time, it is essential to evaluate the classiﬁers on the same datasets. Wang et al. provide two datasets7; one, which we refer to as WANG13, had been used to evaluate the outdated OSAD classiﬁer in [26], and the other, which we call WANG14, had been used to evaluate and compare the k-NN approach in [25]. The WANG13 dataset contains traces of 100 websites with 40 instances each. The websites are based on Alexa’s top sites, where the authors manually removed different localizations of the same site (e.g., google.com and google.de). Obviously, this dataset is only 4 suitable for a closed-world analysis due to its limited size. The WANG14 dataset was built for an open-world analysis. It contains 100 websites with 90 instances each that can be used as foreground class, i.e., as the set of sites monitored by an adversary, or for a closed-world evaluation. This subset was compiled from a list of blocked websites in China, the United Kingdom, and Saudi Arabia. Further, WANG14 includes traces of 9,000 websites drawn from Alexa’s Top 10,000 with one instance each to serve as background data. Note that both datasets include only the index page of each website. Both datasets include information about the direction of each cell and certain management cells (SENDME) were removed using a probabilistic method described in [26]. While WANG13 provides only cell direction and order, WANG14 also includes a timestamp for each cell. This timestamp is necessary for extracting the required characteristics used by the k-NN classiﬁer. In our evaluation we also investigate different layers to extract the information used to generate trace representations (see Section VII-A). Since the required information of these layers is not available in the WANG13 set, we re-recorded the websites used with 100 instances each using our own approach. This allows us to extract all layers of data representation (i.e., also those where SENDME cells are included). Additionally, we can transform our own format to the input format of the k NN classiﬁer and compare our results. We refer to this datasetas ALEXA100.

 B. RND-WWW: An Unbiased Random Sample of the World Wide Web Obtaining a representative sample of web pages visited by typical users is a challenging task. Logs of central inter mediaries, when they are available, e.g., for Internet Service Providers, are generally not publicly available due to privacy concerns. If we were to monitor the web surﬁng behavior of users, the test results could be biased by the selection of users (e.g., students), and the monitoring process itself, e.g., by the Hawthorne effect8. To avoid these issues, we combined several sources, each of which covers a different aspect of anticipated user behavior. We call this dataset RND-WWW. In detail, it is composed of web pages gathered using the following methods: 1) Twitter is a popular social network with more than 300 million average monthly active users that offers micro-blogging services, i.e., users are allowed to post messages with a maximum length of 140 char acters, called tweets. Many people use this service to distribute links to web pages they are currently interested in, and it is to be assumed that many users follow these links. Therefore, Twitter serves as a source of URLs of recent actual interest. Twitter provides an API which enables live access to a stream of randomly-chosen tweets that are currently being posted. From this stream we extracted all HTTP links over a period of three days and resolved the original URL, since Twitter applies a URL shortening service. From this source we were able to gather about 70,000unique URLs of web pages.

 2) Alexa-one-click: As described above, it is uncommon only to visit the index page of a popular website. Instead, users typically also click on links on these pages, or they follow external links that directly lead to subpages of a site, e.g., a link to a particular article on a news website. To simulate this behavior, we loaded the index page of each website in the Alexa Top list of the 20,000 most popular sites and followed a randomly chosen link on this page. We then included the resulting page in our dataset. 3) Googling the trends: Google, by far the most popular search engine, publishes the keywords that were queried most frequently in past years per country as trends9. We used 4,000 trends from Australia, Canada, Germany, Hong Kong, India, Israel, Japan, Singapore, Taiwan, Russia, the United Kingdom, and the USA and queried the corresponding country speciﬁc Google website for these keywords. We then randomly selected a link with a probability of 0.5 from the ﬁrst, 0.25 from the second, 0.125 from the third, and 0.0625 from the fourth and ﬁfth result pages and included the selected target web page inour dataset.

 4) Googling at random: We selected 20,000 English terms at random from the Beolingus German-English dictionary10 and entered them into Google Search. From the results, we selected web pages with the same method as described for Google trends. 5) Censored in China: We added a list of 2,000 web sites blocked in China according to http://greatﬁre.org.  

---

 ## 翻译[5/19]：
 

A. 王等人提供的数据集。为了比较不同方法在分类准确性和计算时间方面的表现，必须在相同的数据集上评估分类器。王等人提供了两个数据集7; 其中一个数据集被我们称之为WANG13，已被用于评估[26]中已过时的OSAD分类器，而另一个数据集被我们称之为WANG14，已被用于评估和比较[25]中的K-NN方法。WANG13数据集包含100个网站的40个实例的痕迹。这些网站基于Alexa的热门网站，其中作者手动删除了同一站点的不同本地化版本（例如，google.com和google.de）。显然，由于规模有限，这个数据集仅适用于封闭世界分析。WANG14数据集是为开放世界分析建立的。它包含100个网站，每个网站有90个实例，可以用作前景类（即敌手监视的站点集），或用于封闭世界评估。这个子集是从中国、英国和沙特阿拉伯封锁的网站列表中编制的。此外，WANG14还包括从Alexa的前10,000个中提取的9,000个网站的痕迹，每个网站一个实例，用作背景数据。请注意，这两个数据集仅包含每个网站的索引页面信息。这两个数据集包含有关每个单元格方向的信息，并使用[26]中描述的概率方法删除了某些管理单元格（SENDME）。虽然WANG13仅提供单元格方向和顺序，但WANG14还包括每个单元格的时间戳。这个时间戳对于提取由k-NN分类器使用的所需特征是必要的。在我们的评估中，我们还调查了不同层以提取用于生成跟踪表示的信息（请参见第VII-A节）。由于这些层的所需信息在WANG13集中不可用，我们使用我们自己的方法重新记录每个使用100个实例的网站。这使我们能够提取所有数据表示的层（即包括SENDME单元格的那些层）。此外，我们可以将我们自己的格式转换为kNN分类器的输入格式，并比较我们的结果。我们将此数据集称为ALEXA100。

B. RND-WWW：全球互联网的无偏随机样本。获取typical用户访问的网页的代表性样本是一项具有挑战性的任务。中央中介机构的日志（如果可用）例如Internet服务提供商，通常由于隐私问题而不公开。如果我们通过监控用户的网络冲浪行为，测试结果可能会受到用户选择（例如学生）和监控过程本身（例如莫斯电焊效应）的影响。为了避免这些问题，我们结合了几个涵盖不同预期用户行为方面的来源。我们将这个数据集称为RND-WWW。具体而言，它由以下方法收集的网页组成：1）Twitter是一个拥有超过3亿平均月活跃用户的流行社交网络，提供微博服务，即用户可以发布最长为140个字符的消息，称为推文。许多人使用此服务分发他们当前感兴趣的网页链接，可以假定许多用户将关注这些链接。因此，Twitter作为最近实际兴趣的URL来源。Twitter提供了一个API，可以实时访问当前正在发布的随机选择的推文流。从这个流中，我们在三天的时间内提取了所有HTTP链接并解析了原始URL，因为Twitter应用了URL缩短服务。从这个来源，我们能够收集大约70,000个独特的网页URL。

2）Alexa-one-click：如上所述，仅访问受欢迎的网站的索引页面是不常见的。相反，用户通常还会点击这些页面上的链接，或者他们会遵循直接导向站点的外部链接，例如新闻网站上的特定文章的链接。为了模拟这种行为，我们加载了Alexa Top中排名前20,000的网站的索引页面，并在此页面上随机选择一个链接。然后，我们将结果页面包括在我们的数据集中。3）浏览趋势：迄今为止最受欢迎的搜索引擎Google根据每个国家最频繁查询的关键字，作为趋势发布。我们从澳大利亚、加拿大、德国、香港、印度、以色列、日本、新加坡、台湾、俄罗斯、英国和美国查询了4000个趋势，并查询了相应国家特定的Google网站以获取这些关键字。然后，我们从第一个页面选择概率为0.5，第二个页面选择概率为0.25，第三个页面选择概率为0.125，第四和第五个结果页面选择概率为0.0625，选取选定的目标网页包含在我们的数据集中。

4）随机搜索：我们从Beolingus德-英词典中随机选择了20,000个英语术语，并将它们输入Google Search。我们使用与Google Trends相同的方法选择网页。

5）中国封锁：我们根据http://greatfire.org上的信息添加了2000个在中国被封锁的网站清单。

## 

---

 ## 原文[6/19]： 

 
 After removing duplicate entries, we were able to combine more than 120,000 unique web pages in total from these sources. In the related research, the set of websites that is considered to be monitored by an adversary (i.e., the fore ground class) is commonly selected from a set of URLs that are known to be actually blocked. In our evaluation, however, we are more interested in investigating whether it is feasible for an adversary to monitor any possible web page. Therefore, we randomly selected a sample of 1% of the pages included in RND-WWW as the foreground class and downloaded 40 instances of each with the methods described in Section V-A. Accordingly, the remaining 99% served as background trafﬁc and were downloaded in one instance each. Finally, the foreground set of RND-WWW consists of 1,125 retrievable web pages, combined from 712 different websites. The sites with the highest frequency are http://facebook.com (88 pages) and http://instagram.com (86 pages). The back ground set is composed of 118,884 unique and accessible web pages distributed among 34,580 different websites. Besides the four websites http://facebook.com, http://youtube.com, http: //instagram.com, and http://tumblr.com, no website is repre sented by more than 2,000 pages. Moreover, 28,644 websites occur only once in RND-WWW, i.e., they are represented by asingle web page.

 5 C. Monitoring Real Tor Trafﬁc For the reasons mentioned in Section II, the website ﬁnger printing attack is typically evaluated against Tor. Therefore, an intuitively representative sample of web pages to be considered for our evaluation are those pages which are actually accessed through the Tor network. Thus, Tor itself serves as source of URLs used for our second dataset, called TOR-Exit. To get access to this data, we operated a public Tor exit node. We ensured that the fast and stable ﬂags were assigned to our node and that its information was thoroughly propagated by the Tor directory service. Hence, it was fully integrated into the operational service of Tor and used in the circuits of real users. We captured HTTP requests from this exit node over a duration of one week. We deliberately limited our selection to plain HTTP trafﬁc as we did not want to interfere with encrypted connections. In general, it is not possible to infer from a HTTP GET request which web page has actually been retrieved, because there is a separate request for each object embedded into a page. Since we are primarily interested in web pages actually visited by Tor users, we extracted URLs in the following manner: HTTP requests typically include a header element called HTTP referer11, which provides the URL of the web page that linked to the resource being requested. Hence, in the case of an embedded object, the referer points to the page containing this object, and if the user followed a link, the referer points to the page containing this link. Thus, the value of the referer serves as suitable source of URLs for webpages that users actually visited.

 From all HTTP requests that contain an HTTP referer12, we include the web page pointed to by the referer in our dataset. From those requests without a referer, we extracted the domain name from which the object is requested and added the website (or, more precisely, its index page) accessible through this domain to our dataset. In both cases, we discarded duplicate entries but deliberately included different web pages belonging to the same website if available. Additionally, we removed query parameters such as session identiﬁers, as these would render a subsequent retrieval impossible. Further, we removed all URLs linking to pure advertisement services, as these are obviously not the target web pages sought by users. With the method described above, we obtained a list of 211,148 unique web pages. The set contains 65,409 unique web domains of which 45,675 occur only once. Each website is represented byfewer than 2,000 web pages.

 Ethical considerations: The collection of URLs for the TOR-Exit dataset does not strictly follow the guidelines for ethical Tor research that were published after we ﬁnished our experiments13. Still, we believe that it is important to know the degree of protection offered by the real Tor network. There can be no better evaluation than using those pages that are actually retrieved via Tor. While running our exit nodes, we made every effort to minimize any potential harm to the users and tried to examine all the risks that may exist. Our scripts extracted and stored only the URLs – without timestamps, traces or any other data. From these we automatically removed all identifying information such as a session identiﬁer. Hence, with the information we stored there is only a minimal risk to harm the anonymity of Tor users. This complies with the recommendations for statistical analyses in the Tor network (except that we did not publicly discuss our algorithms before conducting the experiments) [17]. Furthermore, we will not make this dataset or parts of it publicly available before consulting the Tor research community and an appropriate ethics feedback panel such as the Ethics Feedback Panel forNetworking and Security14.  

---

 ## 翻译[6/19]：
 

在去除重复条目后，我们总共能够从这些来源中合并超过120,000个独特的网页。在相关研究中，常常从已知实际被阻止的URL集中选择被认为是被对手监视的网站集（即前台类）。然而，在我们的评估中，我们更感兴趣的是调查对手是否能够监视任何可能的网页。因此，我们随机选择了RND-WWW中包含在前台类的1%网页的样本，并以V-A节中描述的方法下载了每个样本的40个实例。因此，剩下的99%则用作背景流量，仅下载一个实例。最终，RND-WWW的前台集包含1,125个可检索的网页，来自712个不同的网站。最高频率的网站是http://facebook.com（88个页面）和http://instagram.com（86个页面）。背景集由分布在34,580个不同网站上的118,884个独特且可访问的网页组成。除了http://facebook.com，http://youtube.com，http://instagram.com和http://tumblr.com四个网站外，没有其他网站被超过2,000页所表示。此外，在RND-WWW中只有28,644个网站出现了一次，即它们仅由单个网页表示。

C.监控真实的Tor流量出于第二段中提到的原因，网站指纹攻击通常针对Tor进行评估。因此，对于本次评估要考虑的具有直观代表性的网页样本是那些实际通过Tor网络访问的页面。因此，Tor本身就作为我们第二个数据集TOR-Exit所使用的URL源。为了获得这些数据，我们操作了一个公共Tor出口节点。我们确保为我们的节点分配了快速和稳定的标志，并且Tor目录服务已经彻底传播了其信息。因此，它完全集成到Tor的操作服务中并用于实际用户的电路中。我们在一周的时间内从该出口节点捕获了HTTP请求。我们有意将选择限制为纯HTTP流量，因为我们不想干扰加密连接。通常情况下，从HTTP GET请求中无法推断实际检索了哪个网页，因为每个嵌入页面的对象都有一个单独的请求。由于我们主要关注Tor用户实际访问的网页，因此我们通过以下方式提取URL：HTTP请求通常包含称为HTTP referer的标头元素11，它提供了请求的资源所连接的Web页面的URL。因此，在嵌入对象的情况下，引用者指向包含此对象的页面，如果用户遵循某个链接，则引用者指向包含此链接的页面。因此，引用者的值作为实际受访用户访问的网页的适当URL来源。

从所有包含HTTP referer12的HTTP请求中，我们将引用者所指向的网页包含在我们的数据集中。对于那些没有引用者的请求，我们从其中提取请求对象所需的域名，并将可以通过该域访问的网站（或更准确地说，其索引页面）添加到我们的数据集中。在两种情况下，我们都丢弃了重复的条目，但有意包括属于同一网站的不同网页（如果有的话）。此外，我们删除了所有指向纯广告服务的URL链接，因为这些显然不是用户所寻找的目标网页。通过上述方法，我们获得了一个由211,148个独特网页组成的列表。该集合包含65,409个独特的网域，其中45,675个仅出现一次。每个网站的网页数均少于2,000页。

道德考虑：TOR-Exit数据集的URL收集并不严格遵循我们完成实验之后发布的有关道德的Tor研究指南13。但是，我们认为了解真正的Tor网络提供的保护程度非常重要。使用那些实际通过Tor检索的页面，不存在比这更好的评估方法。在运行我们的出口节点时，我们尽一切努力最小化潜在的用户危害，并试图研究可能存在的所有风险。我们的脚本仅提取和存储URL，而不包括时间戳、跟踪或任何其他数据。从这些URL中，我们自动删除了所有识别信息，如会话标识符。因此，我们存储的信息在最小化对Tor用户匿名性威胁的同时符合Tor网络的建议统计分析（除了我们在进行实验前没有公开讨论我们的算法）。此外，在与Tor研究社区和类似于Networking and Security的适当道德反馈小组（如道德反馈小组）商议之前，我们不会公开发布此数据集或其部分。

## 

---

 ## 原文[7/19]： 

 
 D. Website-Collections We compiled an additional dataset, called WEBSITES, with the aim to investigate whether it is possible to ﬁngerprint a complete website, given that the adversary is only able to use a subset of its pages for training. We assume this to be one of the most realistic attack scenarios. To do this, we selected 20 popular websites that cover different categories (e.g., news sites, social networks, online shops), different layouts, and contents from different regions in the world. Within each of these websites we selected a set of 50 different accessible pages by following links from the index page applying the same method as described for ‘Googling the trends’ in Section IV-B. We then recorded 90 instances of the index page and 15 instances for each of the 50 subpages for all sites. The complete list of websites used in this dataset is available inthe appendix.

V.

 In practice, without loss of generality, we assume that an attacker retrieves a certain amount of relevant web pages by himself as training data for ﬁngerprinting, using the anonymization network that he assumes his victim uses as well. He records the transferred packets with a trafﬁc analyzing tool which provides information about IP layer packets, i.e., the length of the packet, the time the packet was sent or received, the order in which the packets were sent and received, etc. The attacker can make use of various information contained in the dumps to create a proﬁle of each web page, called ﬁngerprint. Later, wiretapping on the victim’s trafﬁc, the attacker tries to match the collected test data to a known ﬁngerprint. Usually, a difference between patterns in training and test data is to be expected due to a number of reasons, e.g., indeterministic packet fragmentation, updates in web pages, varying performance of Tor circuits, etc. Hence, the attacker needs to apply statistical methods to compare the recorded information to the ﬁngerprints and to probabilistically matchit to a certain web page.

 A. Data Collection We accessed the Tor network using the Tor Browser Bundle. This self-contained software package combines a pre conﬁgured Tor client and a stand-alone web browser based on Mozilla Firefox. We used version 3.6.1, which includes patches against website ﬁngerprinting by applying randomized pipelin ing. The intention of TBB is to provide an easily-deployable solution that leaks the minimum amount of information due toidentical browser conﬁgurations.  

---

 ## 翻译[7/19]：
 

D. 网站商品集 我们编制了一个名为WEBSITES的额外数据集，旨在研究是否可能针对完整网站进行指纹识别，假设攻击者只能利用其部分页面进行训练。我们认为这是最现实的攻击场景之一。为此，我们选择了20个涵盖不同类别（例如，新闻网站、社交网络、在线商店）、不同布局和来自世界各地的不同内容的热门网站。在每个网站中，我们根据第IV-B节中“谷歌趋势”的描述，通过从索引页面跟踪链接选择了50个不同的可访问页面的集合。然后，我们为所有站点记录了索引页面的90个实例和每个50个子页面的15个实例。使用的此数据集中的完整网站列表在附录中可用。

V.

 实际上，不失一般性，我们假定攻击者通过使用他认为受害者也使用的匿名化网络自行检索了一定数量的相关网页作为指纹识别的训练数据。他使用流量分析工具记录传输的数据包，该工具提供有关IP层数据包的信息，例如数据包的长度，数据包发送或接收的时间，数据包发送和接收的顺序等。攻击者可以利用转储中包含的各种信息创建每个网页的概要文件，称为指纹。稍后，对于受害者的流量进行监听，攻击者尝试将收集的测试数据与已知的指纹匹配。由于许多原因（例如，不确定的数据包分段、网页更新、Tor电路性能变化等），在训练和测试数据模式之间预计会存在差异。因此，攻击者需要应用统计方法将记录的信息与指纹进行比较，并以概率方式将其匹配到某个网页。

 A. 数据收集 我们使用Tor浏览器包访问Tor网络。这个自包含的软件包组合了一个预配置的Tor客户端和一个基于Mozilla Firefox的独立Web浏览器。我们使用的是3.6.1版本，它包括针对通过应用随机化流水线防范网站指纹识别的补丁。TBB的目的是提供一个易于部署的解决方案，由于相同的浏览器配置而泄漏最少的信息。

## 

---

 ## 原文[8/19]： 

 
 6 We recorded the traces of web pages using tcpdump. We automated the recording using the plugins Chickenfoot15, iMacros, and Scriptish16 and controlled the functionality of Tor with Stem, a Python implementation of the Tor Control Protocol (please note that our Tor controller does not interfere with the regular circuit creation, assignment and fetching of websites). With these methods we were able to automatically retrieve hundreds of thousands of web pages selected from a list of the corresponding URLs. Typically, multiple instances of a single web page are retrieved and analyzed. This is a requirement for training the classiﬁer and for the cross validation we use in our evaluation. We ensured that we never downloaded more than one instance of a single page through the same circuit, as this could distort the evaluation results17. B. Data Extraction and Processing The features we use for ﬁngerprinting are based on packet size, direction, and ordering. It is possible to extract this information at different layers: cell, TLS, and TCP. At the application layer, Tor embeds the encrypted data in ﬁxed-size packets, called cells, with a length of 512 bytes, and cells are further embedded into TLS records. Note that multiple cells may be grouped into a single TLS record. Finally, in the transport layer, TLS records are typically fragmented into multiple TCP packets whose size is bounded by the maximum transmission unit (MTU). Alternatively, several TLS records can be within a single TCP packet. In Section VII-A we provide an evaluation of the different layers of extraction anddiscuss their implication on classiﬁcation.

 From our recorded data, we removed faulty traces that are identiﬁable either by being empty or by an HTTP status code indicating a load error. Further for pages with multiple instances, we removed outliers identiﬁed by the interquartile range, a standard measure to detect outliers in noisy measure ment data [1]. To apply this method, we compute I as the sum of incoming packet sizes for each instance of a web page and the corresponding quartiles. We then remove those instances that do not satisfy the following inequality: Q1 − 1.5(Q3 − Q1) < I < Q3 + 1.5(Q3 − Q1). On average, 5% of the traces were discarded as outliers bythis method.

VI.  

---

 ## 翻译[8/19]：
 

我们使用tcpdump记录了网页的迹象。我们使用了Chickenfoot15、iMacros和Scriptish16插件自动记录，并使用了Stem来控制Tor的功能，Stem是Tor控制协议的Python实现（请注意，我们的Tor控制器不会干扰常规的电路创建、分配和获取网站）。使用这些方法，我们能够自动检索选自相应URL列表的数十万个网页。通常，检索和分析单个网页的多个实例是必需的。这是训练分类器和我们在评估中使用的交叉验证的要求。我们确保我们从同一电路中永远不会下载超过一个页面的单个实例，因为这可能会扭曲评估结果17。 

B.数据提取和处理

我们用于指纹识别的特征基于数据包的大小、方向和顺序。可以在不同的层次上提取此信息：小区、TLS和TCP。在应用程序层面，Tor将加密数据嵌入固定大小为512字节的数据包中，称为小区，并且小区进一步嵌入到TLS记录中。请注意，多个单元可以被组合成单个TLS记录。最后，在传输层中，TLS记录通常被分成多个TCP数据包，其大小受最大传输单元（MTU）的限制。或者，几个TLS记录可以在单个TCP数据包中。在第VII-A节中，我们对不同层次的提取进行评估，并讨论它们对分类的影响。

从我们记录的数据中，我们删除了可通过为空或指示加载错误的HTTP状态代码进行识别的故障迹象。对于多个实例的页面，我们根据四分位距识别异常值，四分位距是检测嘈杂测量数据中异常值的标准方法[1]。为应用此方法，我们计算I作为每个网页实例通入数据包大小的总和和相应四分位数。然后，我们删除不满足以下不等式的实例：Q1−1.5（Q3−Q1）<I <Q3 + 1.5（Q3−Q1）。通过此方法，平均有5％的跟踪被丢弃为异常值。

## 

---

 ## 原文[9/19]： 

 
 The classiﬁer with the highest accuracy known to date is that proposed by Wang et al. [25]. It is based on a k-Nearest Neighbor machine learning approach. The algorithm calculates the distance between data points (here, packet sequences) and classiﬁes a test point based on the class of the k closest training points. The major component of a k-NN classiﬁer is the distance function. Wang et al. use the sum over weighted feature differences as distance metric. Based on prior knowl edge, they manually selected a large set of features, including characteristics such as unique packet lengths, concentration of outgoing packets, or bursts. The weights are learned using an iterative process, where the weights are initialized with random values and then adjusted over several thousands of iterations in order to optimize them. The feature set used for their evaluation consists of almost 4,000 features. We follow a contrasting approach. Instead of manually identifying characteristics that may contain signiﬁcant infor mation about the load behavior, we aim rather to derive our features from an abstract representation that implicitly covers all relevant characteristics. As identiﬁed in [25] and [21], there are four basic features that already contribute signiﬁcant distinctive information: Nin, the number of incoming packets, Nout, the number of outgoing packets, Sin, the sum of incoming packet sizes, and Sout, the sum of outgoing packet sizes. Therefore, we include these four features in our feature set. To characterize the progress of the page load we propose using the cumulated sum of packet sizes as an abstract representation and to sample a ﬁxed number n of additional features fromthis representation.

 When we apply the methods described in Section V-B on our recorded data, we obtain a sequence of packet sizes, where a packet may refer to a raw TCP packet, a TLS record, or a Tor cell, depending on the layer used for data extraction. Given such a trace of packet sizes T = (p1, . . . , pN), where pi > 0 indicates an incoming packet and pi < 0 an outgoing packet, the cumulative representation of this trace is calculated as C(T) = ((0, 0), (a1, c1), . . . , (aN, cN)), where c1 = p1, a1 = |p1|, and ci = ci−1 + pi, ai = ai−1 + |pi| for i = 2, . . . , N. From this representation, we derive n additional features C1, . . . , Cn by sampling the piecewise linear interpolant of C at n equidistant points. This feature Fig. 1: Feature extraction from the cumulative representation of two traces extraction process is illustrated in Figure 1. As a simpliﬁed example we show the cumulative representation of two traces T1 and T2 consisting of |T1| = 18 and |T2| = 14 packets (each of size ± 568 bytes) and the corresponding features Ci for n = 10. With this method, we are able to extract a ﬁxed number of identifying characteristics from traces with varying length. Note that typically N ≫ n, i.e., the trace of a website consists of signiﬁcantly more packets than the number of features that we sample. In Section VII-B we show that n = 100 yields the best trade-off between classiﬁcation accuracy and computational efﬁciency. In the following, we refer to thisﬁngerprinting approach as CUMUL.  

---

 ## 翻译[9/19]：
 

目前被认为准确率最高的分类器是Wang等人提出的[25]。它基于k-最近邻机器学习方法。该算法计算数据点（此处为数据包序列）之间的距离，并根据k个最近训练点的类别对测试点进行分类。k-NN分类器的主要组成部分是距离函数。Wang等人使用加权特征差之和作为距离度量标准。根据先前的知识，他们手动选择了一大组特征，包括唯一分组长度、出站分组的集中度和连续分组等特征。这些权重是使用迭代过程学习的，权重首先初始化为随机值，然后在数千次迭代中进行调整以优化它们。用于评估的特征集包含近4,000个特征。我们采用不同的方法。我们的目标不是手动识别可能包含有关负载行为的显着信息的特征，而是试图从隐含了所有相关特征的抽象表示中导出我们的特征。正如[25]和[21]所确定的，有四个基本特征已经提供了显着的区分信息：Nin，传入数据包数量；Nout，传出数据包数量；Sin，传入数据包大小之和；和Sout，传出数据包大小之和。因此，我们将这四个特征包含在我们的特征集中。为了描述页面加载的进程，我们提出使用数据包大小的累加和作为抽象表示，并从此表示中采样一个固定数量的n个附加特征。

当我们在我们的记录数据上应用V-B部分中描述的方法时，我们得到一个数据包大小的序列，其中一个包可能指的是原始TCP包、TLS记录或Tor单元，这取决于用于数据提取的层。给定这样一个数据包大小的跟踪T=(p1、…、pN)，其中pi>0表示传入数据包，pi<0表示传出数据包，则该跟踪的累计表示计算为C(T)=(（0, 0)，(a1, c1)，...，(aN, cN))，其中c1=p1，a1 = |p1|，并且ci= ci-1 + pi，ai= ai-1 + |pi|，(i = 2，…，N)。从这个表示中，我们通过在n个等距点处对C的分段线性插值进行抽样来得出n个附加特征C1,...，Cn。这个特征提取过程如图1所示。作为一个简化的例子，我们展示了两个跟踪T1和T2的累计表示，它们分别由|T1|=18和|T2|=14个数据包（每个数据包大小为±568个字节）组成，以及n = 10的相应特征Ci。通过这种方法，我们能够从长度不同的跟踪中提取一组固定数量的特征。请注意，通常N≫n，即网站的跟踪数据包明显多于我们采样的特征数量。在VII-B部分中，我们展示了n = 100提供了最佳的分类准确率和计算效率之间的权衡。以下，我们将这种指纹识别方法称为CUMUL。

## 

---

 ## 原文[10/19]： 

 
 7 As a beneﬁcial side-effect of our feature set, ﬁngerprints can be intuitively visualized and compared. In Figure 2 we visualize sample ﬁngerprints derived with our method from the recordings of two popular websites: about.com and google.de. For both websites we recorded 40 instances. As we can see, Fig. 2: Visualized ﬁngerprints of two websites the load behavior of google.de is rather consistent and mainly characterized by a sequence of incoming packets, which is interrupted by a few outgoing packets at certain distinct points in the progress. The ﬁngerprints derived for about.com, a website that publishes articles and videos on various topics, show a greater variation. This site contains several embedded dynamic objects (e.g., images) and their size and position in the trace may vary. Nevertheless, the two websites are characterized by clearly distinctive load behaviors. Our subtle method to represent this load behavior based on the cumulated packet sizes enables the differentiation of ﬁngerprints of these two pages even by the human eye. Obviously, this is not always possible. Therefore, based on our feature set, we collect a set of valid ﬁngerprints and apply a machine learning technique to differentiate them. We use a Support Vector Machine. Since the ﬁngerprints have by deﬁnition a ﬁxed length, we can directly use them as input to train the SVM classiﬁer. To evaluate our approach, we used LibSVM [6] with a radial basis function (RBF) kernel, which is parametrized with parameters c and γ. LibSVM includes a tool to op timize these parameters using cross-validation. It applies a grid search, i.e., various combinations are tested and the one with the best cross-validation accuracy is selected. In its standard implementation the values are chosen from ex γponentially growing sequences c = 2−5, 2−3, . . . , 215 and = 2−15, 2−13, . . . , 23. We adjusted these sequences to c = 211, . . . , 217 and γ = 2−3, . . . , 23, since parameters chosen from these ranges yielded the highest accuracy for our data while reducing the computation time. Before applying the SVM we scale each feature linearly to the range [−1, 1]. This prevents features in greater numeric ranges dominating those in smaller numeric ranges [13]. For all following evaluations in this paper where we do not explicitly mention a different methodology, we always apply our classiﬁer as described inthis section using 10-fold cross-validation.

VII.

 In this section we evaluate our novel website ﬁngerprinting attack in Tor. We ﬁrst identify the optimal layer of data ex traction, then we optimize the parametrization of our method. In Section VII-C we thoroughly compare our approach to the state-of-the-art attack, the k-NN classiﬁer proposed by Wang et al. and show that our method is superior in terms of classiﬁcation accuracy both in the closed and open-world setting as well as regarding computational efﬁciency and scalability. Based on this, we evaluate our approach in different attack scenarios. We show that monitoring a single web page while considering realistic background trafﬁc is doomed to failure. However, in Section IV-D we also provide insights into the scenario, where the attacker aims to monitor complete websites and show that this scenario is still ambitious but more feasible in practice, particularly when considering our improved strategies for training the classiﬁer. A. Layers of Data Extraction As explained in Section V-B, there are three possible layers for data extraction: Tor cell, TLS record, and TCP packet. From the raw TCP data we can extract all three layers. The question we will address now is, which layer provides the most information content with respect to website ﬁngerprinting. Our ﬁrst intuition was that the most information is contained in the TLS layer because only at this layer the dependency of cells that belong together is included. If a record R is still being transmitted in one direction and the transmission of a second record R′ in the opposite direction starts before the TCP packet containing the end of record R is sent, then R′ cannot contain data that is sent in response to the data contained in R. Weillustrate this situation in Figure 3.  

---

 ## 翻译[10/19]：
 

我们的特征集产生的一个有益的副作用是，指纹可以直观地可视化和比较。图2展示了我们的方法从两个流行网站：about.com和google.de的录音中得到的样本指纹。对于这两个网站，我们分别记录了40个实例。如图所示，google.de的负载表现相当一致，主要由一系列传入数据包组成，在进展的某些明显点被一些传出数据包中断。针对about.com，它是一个发布各种主题文章和视频的网站，显示了更大的变化。此网站包含若干嵌入式动态对象（例如图像），它们 在跟踪中的大小和位置可能会变化。尽管如此，这两个网站的负载行为是明显独特的。我们对基于累积数据包大小的微妙方法来表示这种负载行为进行了差异化，使人眼甚至可以区分这两个页面的指纹。显然，并非总是可以实现这一点。因此，基于我们的特征集，我们收集一组有效指纹，并应用支持向量机这种机器学习技术来区分它们。我们使用了支持向量机。由于指纹定义的长度是固定的，因此我们可以直接将它们用作训练SVM分类器的输入。为了评估我们的方法，我们使用了带径向基函数（RBF）内核的LibSVM[6]，其中参数c和γ是经过参数化的。LibSVM包括一个工具，通过交叉验证来优化这些参数。它应用一个网格搜索，即测试各种组合，并选择具有最佳交叉验证精度的参数。在它的标准实现中，这些值从指数增长序列c= 2^-5、2^-3、…，215和γ= 2^-15、2^-13、…，23中选择。我们将这些序列调整为c= 211, …, 217和γ= 2^-3, …, 23，因为从这些范围选择的参数对于我们的数据产生了最高的准确性，同时减少了计算时间。在应用SVM之前，我们将每个特征线性缩放到[-1，1]的范围内。这可以防止数值范围较大的特征主导较小数值范围的特征[13]。对于本文中所有后续评估，如果我们没有明确提到不同的方法，我们总是按照本节所述的方式应用我们的分类器，使用10倍交叉验证。

第七节。本节我们评估我们在Tor中的新颖的网站指纹攻击。我们首先确定数据提取的最优层，然后优化我们方法的参数。在第VII-C节中，我们全面比较了我们的方法与Wang等人提出的最先进的攻击方法：k-NN分类器，并展示了我们的方法在封闭和开放世界设置以及关于计算效率和可扩展性方面的分类准确性优势。基于此，我们评估不同的攻击场景。我们表明，监控单个网页并考虑现实的背景流量注定失败。然而，在第IV-D节中，我们还提供了关于攻击者旨在监控完整网站的情况的见解，并展示这种情况在实践中仍然比较有前途，特别是考虑到我们改进的训练分类器的策略。

A.数据提取的层
正如在第V-B节中所解释的那样，存在三种可能的层可以进行数据提取：Tor cell层，TLS记录层和TCP数据包层。从原始的TCP数据中，我们可以提取这三个层。现在我们要解决的问题是，哪一层对于网站指纹识别提供了最多的信息内容。

我们的第一反应是，最多的信息包含在TLS层中，因为仅在该层中才包括属于一起的单元的依赖性。如果一个记录R在一个方向上仍在传输，而在相反方向上开始传输第二个记录R′ ，并且在包含记录R结束的TCP数据包发送之前，R′不能包含作为响应R中包含的数据发送的数据。我们在图3中说明了这种情况。

## 

---

 ## 原文[11/19]： 

 
 Fig. 3: Example of TLS record reordering The ﬁgure shows four TLS records, their transmission time, and direction. If the records were ordered according to the beginning of each transmission, the resulting sequence would be 1, 2, 3, 4. However, the data contained in record 3 cannot be a response to record 2, since its transmission has started before that of record 2 had ended. As we require an abstract representation of the loading behavior of the website – independent of noise introduced by the transmission – the sequence must be reordered to 1, 3, 2, 4. Hence, we reordered the TLS records extracted from all our traces accordingly. The best classiﬁer known to date, proposed by Wang et al. [26], [25], uses the cell layer. Tor issues a special cell type called SENDME to ensure ﬂow control. These cells are irrelevant for the load behavior of a website and, thus, are a source of noise in the measurement. Wang et al. use a 8 probabilistic algorithm to identify and remove SENDME cells, sometimes leading to a slight improvement in accuracy. This method is also applicable when data is extracted as sequence of TLS records: if a cell is assumed to be a SENDME by the probabilistic algorithm, we can reduce the size of the record containing this cell by 512 bytes. This leads to ﬁve different layers of data extraction, which we evaluate: TCP, TLS, TLSNoSENDME, Cells, and CellsNoSENDME. These layers are illustrated in Figure 4 where it is assumed that Cell 3 is a SENDME. Accordingly, this cell would be removed for the CellsNoSENDME format and TLS record 1 would be shrunk by 512 bytes (indicated by the asterisk) for TLSNoSENDME.

 Tor cells Fig. 4: Layers of data transport used for information extraction To identify the best layer of data extraction we performed several experiments in different settings. Table I exemplarily shows the classiﬁcation accuracy in a closed-world setting for the ALEXA100 dataset using 40, 60, and 90 instances for both classiﬁers, k-NN and our novel CUMUL. As we can see, the layer used for data extraction does not have a signiﬁcant inﬂuence on classiﬁcation accuracy. Surprisingly, throughout our experiments the TLS format did not yield the best results (regardless whether we reorder TLS records as described above or not). The best classiﬁcation accuracy is achieved by extracting data on the Cells layer. However, the differences are small and similar results can be obtained using even the most basic layer of data representation that does not require any post-processing, i.e., TCP. The effect of removing SENDME cells in the TLS and Cells format with the probabilistic method did not achieve consistent results: it either minimally decreases or minimally improves the accuracy. Hence, the effect is negligible and not consistently beneﬁcial. Obviously, this method is not able to reliably detect and remove these management cells. Thus, we omit removing SENDME cells inthe following evaluations.

 In the deﬁnition of the cumulative ﬂow representation C(T) used to derive the features of our classiﬁer (see Section VI), we deﬁned ai to be incremented by |pi|, i.e., the absolute packet size for each packet. However, intuitively, it may be beneﬁcial to increment ai by one instead of the absolute packet size when considering a layer of data extraction with varying packet sizes. Otherwise, it is not possible to differen tiate whether data has been received in form of one larger chunk or several smaller chunks (e.g., TCP packets). We ﬁrst assumed that a large cohesive data block is a distinctive characteristic of a webpage, e.g., an embedded object such as an image. Neglecting this information while processing data may negatively inﬂuence the classiﬁcation accuracy. However, TABLE I: Accuracy of both classiﬁers for the ALEXA100dataset (all values in %).  

---

 ## 翻译[11/19]：
 

图3：TLS记录重新排序示例。该图展示了四个TLS记录、它们的传输时间和方向。如果按照每个传输的开始时间排序，结果会得到1、2、3、4的序列。然而，记录3中包含的数据不能是对记录2的响应，因为它的传输开始时间早于记录2的结束时间。由于我们需要网站加载行为的抽象表示，独立于传输引入的噪声，因此序列必须被重新排序为1、3、2、4。因此，我们按照此顺序重新排序了从所有跟踪中提取的TLS记录。迄今为止已知的最佳分类器是由Wang等人所提出的[26]，[25]，使用的是单元层。Tor发出一种特殊的单元类型称为SENDME以确保流量控制。这些单元与网站的负载行为无关，因此在测量中是噪声的来源。Wang等人使用一种概率算法来识别和删除SENDME单元，有时会导致精度略微提高。当数据被提取为TLS记录序列时，这种方法也是适用的：如果一个单元被假定为SENDME，则我们可以通过512个字节减少包含该单元的记录的大小。这导致五种不同的数据提取层次，我们进行评估：TCP、TLS、TLSNoSENDME、Cells和CellsNoSENDME。这些层次在图4中进行了说明，假定单元3是SENDME。因此，在CellsNoSENDME格式中，该单元将被删除，并且TLS记录1将缩小512个字节（用星号表示）。

图4：用于信息提取的数据传输层级。为了确定最佳的数据提取层次，我们在不同的设置下执行了多个实验。表I示例性地显示了ALEXA100数据集在封闭世界设置下使用40、60和90个实例的分类精度，包括k-NN和我们的新型CUMUL分类器。正如我们所看到的，用于数据提取的层次对分类精度没有显著影响。令人惊讶的是，在我们的实验中，TLS格式没有产生最佳结果（无论是否像上述那样重新排序TLS记录）。提取数据的最佳分类精度是通过Cells层完成的。然而，差异很小，即使使用最基本的数据表示层也可以获得类似的结果，而不需要任何后处理，即TCP。使用概率方法在TLS和Cells格式中删除SENDME单元的效果不一致：它要么轻微减少准确性，要么轻微提高准确性。因此，其效果可以忽略不计并不一致地有益。显然，该方法不能可靠地检测和删除这些管理单元。因此，在以下评估中，我们省略删除SENDME单元的操作。

在用于导出分类器特征的累积流量表示C(T)的定义中（见第VI节），我们将ai定义为对于每个数据包的绝对包大小的增量|pi|。然而，在考虑具有不同包大小的数据提取层时，逐个增加一个ai可能是有益的。否则，无法区分数据是以一个较大的块还是多个较小的块（例如，TCP数据包）的形式接收。首先我们假设一个大的连贯数据块是网页的显著特征，例如一个嵌入式对象，比如图片。在处理数据时忽略此信息可能会对分类精度产生负面影响。然而，表I表明即使不考虑块大小，我们的新型分类器CUMUL也能提供良好的分类性能，并且无论采取哪种数据提取层次，均可轻松实现。从实验结果中，我们发现使用Cells层次比使用TLS或TCP层次更有优势。

## 

---

 ## 原文[12/19]： 

 
 as our experiments indicated, this assumption turned out to be incorrect. Throughout all our experiments with different layers of extraction, incrementing ai by absolute packet sizes yielded signiﬁcantly better results than incrementing ai by one (except for the Cells format, where both operations generate identical features due to equal chunk sizes). It appears that larger data chunks are not a characteristic of the page load behavior (that our attack strives to ﬁngerprint) but rather of the Tor circuit or other network properties (and, hence, is to be treated as noise). B. Optimizing Feature Sampling An important design choice for our novel classiﬁer is the number n of features, i.e., the sampling frequency for the features C1, . . . , Cn. On the one hand, the more ﬁne-grained we sample, the lower is the information loss caused by the sampling. On the other hand, a high number of features negatively inﬂuences the performance in terms of computation time and, thus, scalability for the SVM, because the number of features has a linear inﬂuence on the computational complexity of the underlying optimization problem18. To identify the opti mal trade-off between classiﬁcation accuracy and computation time, we varied the sampling frequency n between 10 and 200. The results are shown in Figure 5. As we can observe, there is no signiﬁcant increase in classiﬁcation accuracy for more than about 100 features for all the three layers of data extraction. Hence, we consider n = 100 as a good choice and use this number of features for the remaining evaluation. Fig. 5: Accuracy as a function of the number of sampled features 9 C. Comparison with State of the Art In this section, we compare the performance of our novel technique to the state-of-the-art approach, i.e., the k-NN clas siﬁer proposed by Wang et al. [25], that has been shown tooutperform prior approaches.

 1) Closed World: We ﬁrst compare the performance in the closed-world scenario using the ALEXA100 dataset and the 100 websites provided as foreground in the WANG14 dataset. Although the closed-world scenario is generally not realistic, it is suitable for comparing the classiﬁcation performance of different classiﬁers. In this context, the accuracy, which is deﬁned as the fraction of correct classiﬁcations (positive and negative) among the total number of cases examined, is a suitable metric. Note that in the case of unbalanced datasets that we have to face in the open-world setting, the accuracy is practically meaningless and, thus, other metrics are to beevaluated.

 TABLE II: Accuracy of both classiﬁers for the WANG14 dataset(all values in %).  

---

 ## 翻译[12/19]：
 

正如我们的实验所示，这个假设被证明是不正确的。在我们进行不同抽取层次的实验中，通过绝对数据包大小增加ai比通过增加1的结果要好得多（除了在Cells格式中，由于均等的数据块大小，这两个操作产生相同的特征）。看起来，更大的数据块不是页面加载行为（我们的攻击试图指纹识别）的特征，而是Tor电路或其他网络属性的特征（因此，应将其视为噪音）。B. 优化特征采样 对于我们的新分类器，一个重要的设计选择是特征数n，即对特征C1，…，Cn的采样频率。一方面，我们采样越细粒度，因采样而引起的信息损失就越低。另一方面，高数量的特征会有负面影响，包括计算时间和SVM的可扩展性，因为特征数量对底层优化问题的计算复杂度有线性的影响[18]。为了确定分类准确度和计算时间之间的最佳权衡，我们将采样频率n在10到200之间进行了变化。结果如图5所示。我们可以观察到，在所有三个数据抽取层次中，对于超过约100个特征，没有明显的分类准确度提高。因此，我们认为n=100是一个良好的选择，并在后续的评估中使用该数量的特征。图5:所采样特征数目的准确度9 C. 与现有技术的比较 在本节中，我们将我们的新技术的性能与最先进的方法进行比较，即由Wang et al. [25]提出的k-NN分类器，已经显示出优于之前的方法。

1）封闭世界：我们首先使用ALEXA100数据集和WANG14数据集中提供的100个网站作为前景，比较封闭世界情况下的性能。虽然封闭世界场景通常不是真实的，但它适合比较不同分类器的分类性能。在这个情况下，准确度是一个合适的度量标准，它定义为在检查的总案例中正确分类（正面和负面）的比例。请注意，在我们在开放世界中面临的不平衡数据集的情况下，准确度实际上是没有意义的，因此必须评估其他指标。

表II：WANG14数据集中两种分类器的准确度（所有值以%计算）。

## 

---

 ## 原文[13/19]： 

 
 The results for the ALEXA100 dataset are shown in Table I, where we also differentiate between all evaluated layers of data extraction. In Table II we show the results for the WANG14 dataset. Here, for reasons of clarity we only show the results for that data format for which each classiﬁer performed best. As we can see, our CUMUL classiﬁer, which is based on only 104 intuitive features – compared to 3736 synthetically gener ated features for k-NN – generally achieves a greater accuracy than k-NN on both datasets. However, the improvements are marginal, at about only 1 to 2 percentage points. Note that the accuracy obtained using both approaches is already remarkably high. Therefore, it is questionable whether further efforts to improve the accuracy on such datasets are reasonable. This could ﬁnally lead to the problem that the features consider unique characteristics of the particular websites contained in these sets and, hence, results might no longer be generalizable. This problem is related to overﬁtting, i.e., the effect of a model describing observed noise rather than the underlying relationship. Additionally, we identiﬁed a difference in the implementation of the cross-validation methodology of both classiﬁers that makes a fair comparison difﬁcult: while the k-NN uses 60 instances of each foreground page for weight learning and 30 instances for testing, the SVM performs an internal cross-validation to optimize kernel parameters, which uses all 90 instances of each foreground page. Besides this basic separation difference, the selection of testing instances might also differ. To make the results more comparable, we modiﬁed the implementation of both approaches accordingly for the open-world scenario described in the next section. 2) Open World: For the comparison in the more realistic open-world scenario, we used the complete WANG14 dataset, consisting of 100 foreground pages (90 instances each) and 9,000 background pages (1 instance each). To avoid the cross validation issue described above, we implemented an addi tional, enclosing 10-fold cross-validation. This step ensures that the data used for training and testing in each fold is exactly the same for both classiﬁers while all 90 × 100 foreground and 9000 × 1 background instances are used exactly once for testing. Thus, within each fold, we select 90% of the data (100 × 81 foreground and 8100 × 1 background instances) for training, i.e., weight learning of the distance function and calculating differences to the testing point in the case of k NN and optimizing the kernel parameters in the case of the SVM. Correspondingly, we obtain 1800 testing predictions for each fold and classiﬁer. For the comparison we consider two scenarios: multi-class and two-class. The basic difference is whether each foreground page is treated as a different class (multi-class) or the whole set of monitored pages forms a single class (two-class). In the two-class scenario, the chances for false positive classiﬁcations are lower, because confusion within the foreground (i.e., a particular monitored page is recognized as being a different monitored page) is irrelevant. Note that the difference in both scenarios is not a matter of optimizing a different classiﬁer (as this would falsely imply that monitored pages have a similarity that distinguishes them from the background, which is not true in practice), but rather a matter of counting, i.e., whether to count confusion between two foreground pages as false positive or not. TABLE III: Results for the open-world scenario of both classiﬁers using the WANG14 dataset (all values in %). The results are shown in Table III. For the multi-class scenario, we can see that our method clearly outperforms the k-NN with a TPR, which is 7 percentage points higher while achieving a lower FPR. Further we observe that the TPR and FPR of the modiﬁed k-NN implementation perfectly match the ROC curve shown in [25]. Thus, we conclude that our modiﬁcation did not inﬂuence the classiﬁcation accuracy. It would be interesting to compare complete ROC curves of both classiﬁers instead of single values. However, we have to leave this for future work due to the enormous computation timerequired.  

---

 ## 翻译[13/19]：
 

ALEXA100数据集的结果如表I所示，我们还区分了所有进行数据提取的评估层。在表II中，我们展示了WANG14数据集的结果。在这里，为了清晰起见，我们仅展示了每个分类器表现最佳的数据格式的结果。正如我们所看到的，我们的CUMUL分类器，仅基于104个直观特征-而k-NN则基于3736个合成特征，通常在两个数据集上都比k-NN实现更高的精度。但是，这些改进很微小，仅有1到2个百分点。请注意，两种方法获得的精度已经非常高。因此，是否进一步努力提高这些数据集的精度是值得商榷的。这最终可能导致所考虑的特征考虑到包含在这些数据集中的特定网站的独特特点，因此结果可能不再具有推广性。这个问题与过拟合有关，即模型描述观察到的噪音而不是潜在关系的效果。此外，我们还发现两种分类器交叉验证方法的实现存在差异，这使得公正比较变得困难：k-NN使用每个前景页面的60个实例进行权重学习和30个实例进行测试，而SVM执行内部交叉验证以优化核参数，这使用了每个前景页面的所有90个实例。除了这个基本的分离差异之外，测试实例的选择也可能不同。为了使结果更具可比性，我们相应地修改了下一节中描述的开放世界情景中的两种方法的实现方式。2）开放世界：为了比较更现实的开放世界情景，我们使用由100个前景页面（每个页面90个实例）和9,000个背景页面（每个页面1个实例）组成的完整的WANG14数据集。为避免上述交叉验证问题，我们实施了另一个封闭的10折交叉验证。该步骤确保在每个分类器的各个折叠中，用于训练和测试的数据完全相同，同时90 x 100个前景和9000 x 1个背景实例被精确地用于测试。因此，在每个折叠中，我们选择90％的数据（每个前景页面100 x 81个前景和8100 x 1个背景实例）进行训练，即k NN中的距离函数权重学习和测试点差异计算优化内核参数的情况SVM。相应地，我们为每个折叠和分类器获得1800个测试预测。为了比较，我们考虑了多类和两类两种情况。基本区别在于是否将每个前景页面视为不同的类（多类）或整个监视页面集合形成单个类别（两类）。在两类情况下，误报分类的机会较低，因为前景中的混淆（即识别为不同监视页的特定监视页）是无关紧要的。请注意，两种情况的区别不是为了优化不同的分类器（因为这将错误地暗示被监控的页面有一个将它们与背景区别开来的相似性，这在实践中不是真的），而是一个计数问题，即是否将两个前景页面之间的混淆视为误报或不计。表III：使用WANG14数据集的两种分类器在开放世界情境下的结果（所有值均为％）。表III中显示了结果。对于多类情况，我们可以看到我们的方法比k-NN更胜一筹，其TPR高出7个百分点，同时实现较低的FPR。此外，我们观察到，修改后的k-NN实现的TPR和FPR与[25] 中所示的ROC曲线完全匹配。因此，我们得出结论，我们的修改没有影响分类准确度。比较两种分类器的完整ROC曲线而不是单个值将是有趣的。但是，由于需要大量计算时间，我们必须将其留给将来的工作。

## 

---

 ## 原文[14/19]： 

 
 For a realistic adversary such as a state-level censor, con fusion within the monitored pages is not a problem. Therefore, the results in the two-class scenario are applicable in practice. Here, we see the same relation in the results, i.e., our approach achieves a clearly higher TPR for a lower FPR. Interestingly, we observe a signiﬁcant decrease of false positives when considering only two classes. This means that most of the false positives in the multi-class scenario have been caused by intra-foreground confusion, although it is reasonable to expect signiﬁcantly more background pages to be mistakenly classi ﬁed as monitored. We identiﬁed the reason for this observation in the compilation of websites used in the WANG14 dataset. This set contains multiple pairs of sites that are very similar, for instance, two different localizations of Yahoo19. Obviously, such similarities confuse any classiﬁcation approach and it is a subject for debate to label such sites as one class. 10 Fig. 6: Comparison of runtimes for the different approaches (y-axis scaled logarithmically) Besides the detection accuracy we also compared the computational performance in terms of runtimes for our novel approach and the k-NN. To do this, we selected 100 foreground pages at random from the RND-WWW dataset and performed an open-world evaluation using both approaches under identical settings (same machine, same cross-validation methodology). Figure 6 shows the average computation times required by each method for varying background set sizes. Note that libSVM has a buit-in parallelization functionality while the implementation of Wang et al. does not support parallelization. Therefore, we evaluated our approach with and without this option enabled. The results are unambiguous: our approach is faster by several orders of magnitude (note that the y-axis is scaled logarithmically) and scales signiﬁcantly better for increasing dataset sizes. Moreover, the performance of our approach can additionally be increased by enabling parallelization (particu larly for optimizing the kernel parameters) and the runtime of our method could be further improved without signiﬁcant loss of accuracy, by reducing the feature sampling frequency, see Section VII-B. We assume that the computational costs for the k-NN classiﬁcation (in particular, the method used to adjust the weights of the distance function) increase faster with a growing number of instances. Due to its immense computational costs we have to omit further evaluations on large-scale datasetsusing this classiﬁer.

 Taking all results into account, we conclude that our novel website ﬁngerprinting attack outperforms all previously proposed methods both in the closed and the open-world scenario, while obtaining such results with signiﬁcantly less computational effort and, thus, much faster. Hence, we are equipped with the best website ﬁngerprinting technique known to date that we now use to investigate how severe the WFPattack is in reality.

 D. Webpage Fingerprinting at Internet Scale We now address the fundamental question whether the website ﬁngerprinting attack scales when applied in a realistic setting, i.e., whether it is possible to detect a single webpage in real-world Internet trafﬁc in general and in Tor trafﬁc in particular. To do this, we apply and optimize a separate classiﬁer for each page in the foreground class, i.e., the set of pages to be monitored. We argue, and our experiments conﬁrm this claim, that this is the dominant strategy for an attacker in order to increase the probability for success. We later discuss the implications of this strategy for the scenario where the adversary aims to monitor multiple web pages. We concentrated on the open-world scenario, where the universe of web pages a user may visit is not artiﬁcially restricted to a small set. Therefore, this evaluation allows us to investigate whether our novel attack allows an attacker to ﬁngerprint the load behavior of a single page within realistic Internet noise. We selected all 1,125 available foreground pages in RND-WWW. To evaluate whether the attack scales, we investi gated increasing sizes of the background class. Concretely, we evaluate the attack for a varying size b of background pages, with b ∈ {1000, 5000, 9000, 20000, 50000, MAX}, where MAX corresponds to the maximum available background set size. Further, we evaluated two scenarios. Assume the classiﬁer is trained on a particular web page w of our foreground set, where w is not contained in the background class. We then considered two types of background trafﬁc: • unﬁltered: the background class remains unchanged. The question we address here is whether it is possible to monitor an arbitrary webpage in general. • ﬁltered: all other web pages w′, which belong to the same web site as w, are removed from the background set. This is to get an intuition for the upper bound ofthe detection efﬁcacy.  

---

 ## 翻译[14/19]：
 

对于如国家级审查员一样的现实对手来说，被监视页面中的混淆不是问题。因此，两类情况下的结果在实践中是适用的。在这里，我们看到结果中的相同关系，即我们的方法在较低的FPR时实现了明显更高的TPR。有趣的是，当仅考虑两类时，我们观察到假阳性数量显著减少。这意味着多类情况下的大部分假阳性都是由前景内混淆引起的，尽管可以合理地预期将背景页面错误地分类为被监视的页面。我们在WANG14数据集中使用的网站编译中确定了此观察结果的原因。该数据集包含多个非常相似的网站对，例如，Yahoo19的两个不同本地化版本。显然，这种相似性会困扰任何分类方法，并且是否将此类网站标记为一类是值得探讨的问题。

图6：不同方法的运行时间比较（y轴对数缩放）

除检测精度外，我们还比较了我们的新方法和k-NN的计算性能，即在相同设置下（同一台计算机、同一交叉验证方法）选取RND-WWW数据集中的100个前景页面，并使用两种方法进行开放世界评估。 图6显示了每种方法所需的平均计算时间，其中包含不同背景集大小的变化。请注意，libSVM具有内置的并行化功能，而Wang等人的实现不支持并行化。因此，我们使用和未使用此选项对我们的方法进行了评估。结果是明确的：我们的方法比k-NN快几个数量级（请注意，y轴按对数缩放），而且对于不断增加的数据集大小，我们的方法明显更具可扩展性。此外，我们的方法的性能可以通过启用并行化（特别是为了优化核参数）而进一步提高，并且我们的方法的运行时间可以通过减少特征采样频率而进一步改善，具体见 VII-B 节。我们假设 k-NN 分类的计算成本（特别是用于调整距离函数权重的方法）随着实例数量的增加而增加。由于其巨大的计算成本，我们必须省略使用此分类器进行大规模数据集的进一步评估。

考虑所有的结果，我们得出结论，我们的新型网站指纹攻击在封闭和开放世界场景下优于所有先前提出的方法，同时具有更少的计算工作量和更快的速度。因此，我们拥有迄今为止已知的最佳网站指纹技术，现在我们将使用它来调查WFP攻击在现实中的严重程度。

D. 互联网规模的网页指纹识别
现在，我们回答一个基本问题，即在实际设置中应用网站指纹攻击是否具有可扩展性，即是否可能在一般情况下和特别是在Tor流量中检测到单个网页。为此，我们为前景类中的每个页面应用和优化一个单独的分类器，即要监视的页面集。我们认为，并且我们的实验证实了这一点，这是攻击者的主要策略，以增加成功的可能性。我们随后讨论了这种策略对攻击者旨在监视多个网页的场景的影响。我们集中研究了开放世界场景，其中用户可能访问的网页宇宙不受人为地限制在一个小集合中。因此，这种评估允许我们调查我们的新型攻击是否允许攻击者在实际互联网噪声中对单个页面的负载行为进行指纹识别。我们选择了RND-WWW中所有1,125个可用的前景页面。为了评估攻击是否可扩展，我们研究了背景类的规模增加。具体而言，我们评估了变化的背景页面大小b，其中b ∈ {1000, 5000, 9000, 20000, 50000, MAX}，其中MAX对应于最大可用背景集大小。此外，我们评估了两个情景。假设分类器是针对前景集w的特定网页训练的，其中w不包含在背景类中。然后，我们考虑了两种类型的背景流量：•未过滤：背景类保持不变。我们要解决的问题是是否可能通常监视任意网页。•过滤：所有其他属于与w相同的网站的网页w ′ 从背景集中删除。这是为了获得检测效果的上限的直观感受。

## 

---

 ## 原文[15/19]： 

 
 The difference in these scenarios is whether other web pages that belong to the same website are treated as false positives or not. The ﬁltered scenario provides an upper bound to the detection efﬁcacy, because it assumes that users do not visit other pages of the monitored site. The unﬁltered scenario shows whether a particular page of a site can be monitored while users may visit other (unmonitored) pages of that site. The unﬁltered scenario is more difﬁcult in practice as it is to be expected that different pages of the same site exhibit a similar load pattern and, thus, confuse the ﬁngerprinting method. It follows that in the ﬁltered scenario we have to compile a different background set for each considered page of the foreground class, since different instances must be removed. Therefore, the size MAX may vary slightly throughout ourevaluation.

 The accuracy, i.e., the probability of a true result (either true positive or true negative), cannot serve as indicator of the adversary’s success in practice, since the sizes of the foreground and the background class are heavily unbalanced. We illustrate this with an intuitive example. Assume we train the classiﬁer for one foreground page (i.e., using 40 instances) and use a background class consisting of 1,000 pages (with one instance each). In this case, it is trivial to deﬁne a classiﬁer that achieves an accuracy above 96%: a classiﬁer that rejects any instance, i.e., which classiﬁes any given instance as background, classiﬁes 1,000 out of 1,040 cases correctly, i.e., 96.15%. This effect becomes even more pronounced when the size of the background class is increased, e.g., to 99.9% for 50,000 background instances. Therefore, we use two metrics that are commonly applied in similar domains: precision and recall. The recall20 corresponds to the probability that access to a monitored page is detected. In the related work, the quality of a website ﬁngerprinting technique had mostly been evaluated 11 using only the True Postive Rate and False Positive Rate. These metrics are at ﬁrst glance intuitive as they express both the fraction of accesses to monitored pages that were detected (TPR) and the probability of false alarms (FPR). However, a low FPR leads to incorrect interpretations if the prior, i.e., the fraction of monitored pages within the total number of visited pages, is not taken into account. This effect is known as base rate fallacy. Recent works [4], [14] started to considerthis fact.

 The precision is deﬁned as the number of true positives divided by the number of positive test outcomes. This metric takes account of the prior and the actual size of the universe. It corresponds to the probability that a classiﬁer is actually correct in its decision when it claims to have detected a mon itored page. Which metric is more important depends on the objective of the adversary. If the adversary wants to uniquely identify the user that has visited a particular monitored web page, then the precision is more important, because otherwise many innocent users may be suspected. If the primary goal is to restrict the set of users to those that may have visited monitored web pages, then the recall is more important, because the probability that accesses to monitored pages are detected at all is more important than the number of false alarms. Ideally, from the adversary’s perspective, precision and recall both should be equal or close to one. In this case, he ensures that all users visiting monitored pages are detected and the detection is practically always correct. We calculated these Fig. 7: RND-WWW: CCDF of precision for increasing back ground set sizes two metrics for both scenarios (ﬁltered and unﬁltered) and each web page considered as being monitored, i.e., each page contained in the foreground class. We repeated this calculation for different values of b as described above and used 10-fold cross-validation in each run. The results are shown in Figures 7 and 8, visualized using complementary cumulative distribution functions21 (CCDFs). We can see that both metrics, precision and recall, clearly decrease for increasing background set sizes. In the ﬁltered scenario, more than 80% of the foreground pages are detectable with a precision greater than 0.8 if the considered universe is small, i.e., b = 1, 000. However, only 40% achieve at least this precision when the background class is extended to the maximum number available. Moreover, for fewer than 20% of the pages does the classiﬁer achieve aprecision greater than 0.9.  

---

 ## 翻译[15/19]：
 

这些情况的不同之处在于是否将属于同一网站的其他网页视为误报。被过滤的场景提供了检测效果的上限，因为它假定用户不访问被监视站点的其他页面。未经过滤的场景则显示了在用户可能访问该网站的其他（未被监视的）页面的情况下是否可以监视该网站的特定页面。未经过滤的场景在实践中更加困难，因为同一站点的不同页面可能会展示出类似的负载模式，从而混淆指纹识别方法。因此，在经过过滤的场景中，我们必须为前景类别中的每个考虑的页面编译一个不同的背景集，因为必须移除不同的实例。因此，在我们的评估中，大小MAX可能会略微发生变化。

准确率即真实结果（真正率或真负率）的概率，不能作为实践中对对手成功的指示器，因为前景类别和背景类别的大小存在严重的不平衡性。我们用一个直观的例子来说明这一点。假设我们为一个前景页面训练分类器（即使用40个实例），并使用由1,000个页面（每个页面仅有一个实例）组成的背景类别。在这种情况下，很容易定义一个分类器，使其准确率在96%以上：拒绝任何实例的分类器，即将给定的任何实例分类为背景类别，将1,000个案例中的1,040个正确分类，即96.15%。当增加背景类别的大小时（例如，将背景实例的占比提高到50,000的99.9%），这种效应会变得更加显著。因此，我们使用在类似领域中通常应用的两个指标：查准率和查全率。recall20对应于监视页面被检测到的概率。在相关工作中，网站指纹识别技术的质量大多使用真正率和假正率来评估。这些指标乍一看很直观，因为它们表示被监视页面访问的比例中被检测到的比例（TPR）和虚警的概率（FPR）。然而，如果不考虑先验条件，即被监视页面在访问页面总数中的占比，则低FPR将导致错误的解释。这种效应被称为基础概率谬误。最近的工作开始考虑这一事实。

查准率定义为真正例数除以所有测试结果为正的数目。该度量考虑了先验条件和宇宙的实际大小。调用程序在声称检测到一个被监视页面时实际上正确决策的概率对应于该度量。哪个指标更重要取决于对手的目标。如果对手想要唯一地识别访问特定被监视网页的用户，则精度更重要，因为否则可能会怀疑许多无辜的用户。如果主要目标是限制集合为那些可能访问被监视网页的用户，则查全率更重要，因为被检测到的访问被监视页面的概率比虚警的数量更重要。理想情况下，从对手的角度来看，精度和查准率应该都接近于或等于1。在这种情况下，他确保所有访问受监视页面的用户都被检测到，检测基本上始终是正确的。我们针对每个页面均视为受监视（即包含在前景类别中）的场景（过滤和未过滤）计算了这两个度量，并重复了上面所述的b的不同值以及每个运行中的10倍交叉验证。结果显示在图7和图8中，使用互补累积分布函数21（CCDF）进行可视化。我们可以看到，精度和查准率两个指标随着背景集大小的增加而明显下降。在被过滤的场景中，如果所考虑的宇宙比较小（即b = 1,000），则80％以上的前景页面检测精度大于0.8。然而，在将背景类别扩展到最大数量时，只有40％的页面达到至少此精度。此外，对于不到20％的页面，分类器实现高于0.9的精度。

## 

---

 ## 原文[16/19]： 

 
 Fig. 8: RND-WWW: CCDF of recall for increasing background set sizes For the recall we make similar, yet even more remark able observations. If we assume an attacker whose primary objective is to cover as many visits to monitored pages as possible, hence, who is interested in a high recall, and let the threshold of the recall, which identiﬁes a foreground page as being ‘detectable’ (ignoring false alarms) be 0.5, i.e., a page is assumed to be detectable if the classiﬁer detects access in at least half the cases. Such an adversary is able to detect almost each web page if b =1,000 but only less than 50% of the pages for b =111,884. If we further assume a page to be ‘reliably detectable’ if the recall is greater than 0.9, then the attacker is still able to reliably detect 60% of the pages for b =1,000. However, if b is increased to the maximum value in our evaluation, the rate of reliably detectable pages drops below 5%. What is more, recall that this scenario is simpliﬁed to the beneﬁt of the attacker and is to provide an intuition for the upper bound, as it assumes the background to be ﬁltered,which is not possible in practice.

 In general, our assumptions regarding the more realistic unﬁltered scenario are conﬁrmed. When other pages of the website that the considered foreground page belongs to, are to be classiﬁed as true negatives, precision and recall decrease further. Obviously, in this case the classiﬁer mistakenly con fuses more monitored pages with noise and vice versa. Fig. 9: TOR-Exit: CCDF of precision and recall for increas ing background set sizes We performed the open-world evaluation also using the TOR-Exit dataset as background noise in order to ﬁnd out whether there is a fundamental difference in the results when the sets of background pages are compiled from pages that are actually visited using Tor instead of being randomly chosen or particularly popular. The results are shown in Figure 9. This dataset is considerably larger, leading to MAX > 12 200,000. Consequently, the required computation time for each foreground page and background set size is increased. To keep the overall computation feasible, we limited the number of foreground pages to 850. Thus, the fractions calculated for the CCDFs are not directly comparable to those in Figure 7. However, the general trend in the results remains unchanged. The fraction of foreground pages for a ﬁxed value of precision and recall steadily decreases with increasing background sizes. In summary, we see that for each step of increasing the number of background pages, i.e., the size of the considered universe, both precision and recall decline. Hence, it is to be assumed that this trend continues for further increases in universe size. Taking into account that MAX is still vanishingly low compared to the number of pages in the world wide web, we can conclude that the described attack does not scale. Recall that this evaluation is still overestimating the success probability of a real-world adversary, because due to using 10-fold cross-validation, we conceded him to train the classiﬁer using 90% of the entire universe. In practice, this is not possible, e.g., for the world wide web and, thus, only a small subset can be selected for training. To investigate this, we experimentally ﬁxed a subset of 5,000 pages (a number that is assumed to be closely sufﬁcient in the related work [27]) for training and only increased the size of the set usedfor testing.

 Fig. 10: CCDF of the minimum number of background pages that are mistakenly confused with the considered foreground page for different background set sizes. The number of trainingsamples is ﬁxed to 5,000.

 Obviously, this reduces the classiﬁcation accuracy. To give an impression of the practical implications, in Figure 10 we show the absolute minimum number of background pages that are confused with the considered foreground page. Here, we used 1,000 pages from RND-WWW as foreground and the plot shows the CCDF for that fold of a 4-fold cross-validation, that yielded the minimum number of confusions (i.e., the best result from the adversary’s perspective). 2% of this foreground set, i.e. 20 web pages, for b = 20, 000 (i.e., in dimensions of universe size considered in the related work), do not have a single confusion. But if b is increased to 100,000, each of the 1,000 considered foreground pages is mixed up with at least 8 pages in the background. This demystiﬁes the assumption, that there may be web pages that are particularly easy to ﬁngerprint: as we show there is not a single page for which no confusinglysimilar page exists in a realistic universe.  

---

 ## 翻译[16/19]：
 

图8：RND-WWW：随着背景集大小的增加，召回率的CCDF。针对召回率，我们得出类似但更令人注目的观察。如果我们假设攻击者的主要目标是覆盖尽可能多的访问受监测页面，因此他对高召回率感兴趣，并且将召回率的阈值设置为0.5，即分类器检测到访问至少在一半的情况下，假设将一个前景页面视为“可检测”（忽略误报）。如果b = 1,000，则这样的攻击者能够检测几乎每个网页，但如果b = 111,884，则只能检测少于50％的页面。如果我们进一步假设当召回率大于0.9时，将一个页面视为“可靠检测”，则攻击者仍能够可靠检测60％的页面，当b增加到我们评估的最大值时，可靠检测页面率低于5%。而且，要记住，这种情况是为了简化攻击者的界限，以便提供对上限的直觉，它假设背景被过滤，但实际上是不可能的。

总的来说，我们关于更现实的未过滤情况的假设得到了确认。当网站的其他页面被归类为真负时，精确性和召回率进一步降低。显然，在这种情况下，分类器会将更多的受监测页面与噪音相混淆，反之亦然。

图9：TOR-Exit：CCDF，用于增加背景集大小的精度和召回率。我们还使用TOR-Exit数据集作为背景噪声进行了开放世界评估，以查找当背景页面集合由实际使用Tor访问的页面编译时，与从随机选择或特别流行的页面编译背景页面集合时的结果是否有根本区别。结果如图9所示。该数据集相当大，导致MAX> 12 200,000。因此，每个前景页面和背景集大小所需的计算时间增加。为了使总体计算可行，我们将前景页面的数量限制为850。因此，计算CCDF计算的分数与图7中不直接可比。但是，结果的总体趋势保持不变。对于固定的精度和召回率值，与增加背景页面数量（即考虑的宇宙大小）一样，前景页面的比例稳步下降。因此，可以假定这种趋势会随着宇宙大小的进一步增加而继续。考虑到MAX仍然远低于世界范围内的页面数量，我们可以得出结论，所描述的攻击不具备可扩展性。要注意的是，这种评估仍然高估了现实世界攻击者的成功概率，因为由于使用10倍交叉验证，我们允许他使用90％的整个宇宙来训练分类器。实际上，这是不可能的，例如对于世界范围内的网站，因此只能选择少量网页进行训练。为了研究这个问题，我们实验性地选择了5,000个页面的子集（这个数字在相关的工作[27]中被认为足够接近），用于训练，并仅增加用于测试的集合的大小。

图10：考虑到训练样本数量固定为5,000，误解前景页面的背景页面最少的CCDF，用于不同背景集大小。显然，这降低了分类精度。为了给出实际影响的印象，在图10中，我们展示了与来自RND-WWW的1,000个页面的前景一起使用，该图显示了4倍交叉验证的那个折叠的CCDF，其中包含最少的混淆数量（即攻击者的最佳结果）。如果b增加到100,000，则有至少8个页面与每个考虑的前景页面混淆。这个结果揭示了这样一种假设的神秘感，即可能有特别易于被指纹识别的网页：正如我们所显示的，在现实宇宙中没有一个单独的页面，没有相似的页面可以被混淆。

## 

---

 ## 原文[17/19]： 

 
 E. Detection of Websites We have shown in the previous section that ﬁngerprinting a single web page is not feasible. We now investigate another, more realistic attack scenario where the adversary aims to monitor a complete website. In this context the strategy de scribed to be optimal in the webpage scenario above would be disadvantageous, since training a separate classiﬁer for each page of a site dramatically increases the number of false positives to be expected (because the classiﬁcation consists of multiple decisions while a single false decision is enough to confuse two websites). In general a web page may only be altered due to dynamical content. It has already been shown by Juarez et al. [14] that page updates have a profound impact on the classiﬁcation accuracy. Websites, however, can additionally change due to adding new pages or removing existing pages. Besides, for many websites it is practically infeasible to ﬁngerprint each page contained due to their enormous amount, e.g., facebook.com. Hence, the objective to create a website classiﬁer is more challenging. Therefore, we now analyze which attack strategies an adversary may follow and analyze their efﬁcacy. To be realistic in this regard, we only concede the attacker to use a subset of available pagesfor training of a website.

 First, we investigate the most simple scenario to get an intuition about the complexity of the problem in relation to a webpage classiﬁer. To do this, we performed the following experiment. In two disjoint closed-world settings, we aimed to differentiate between 20 websites in our WEBSITES dataset. In case (a), a website is only represented by multiple instances of its index page as it is typically evaluated in the related work. This corresponds to the problem of classifying web pages. In case (b), a site is given by a subset of its webpages. In both cases we used 51 instances per class (website), i.e., all available instances of the index page in case (a) and 51 different other non-index pages in one instance each in case (b). Figure 11 shows the confusion matrices for both cases using heatmaps. As we can see, websites can be “perfectly” separated based on their index page (accuracy: 99%). Contrary, the classiﬁcation based on a subset of webpages is much less accurate even in such a tiny closed-world setting (accuracy: 82%). We now consider different attack strategies an attacker may apply to improve the website classiﬁcation accuracy. The ﬁrst strategy assumes that the index page is particularly characteristic for the whole website. Therefore, we included it (in 20 instances) in the training set of the classiﬁer in case (b). However, the assumption turned out to be false: the presence or absence of the index page during the training does not have impact on the resulting accuracy. Second, having only one instance per non-index webpage may not be enough and thus could deteriorate the resulting accuracy. However, even using 15 instances for each of the 50 webpages per site did not improve the classiﬁcation signiﬁcantly (accuracy: 85.99%). The third strategy does not only consider one class per website, but instead classiﬁes each page within a site seperately to take accout of their diversity. Then, confusion within the classes representing different webpages of the same website is ignored and counted as true positive. The classiﬁer built according to this strategy yielded even a slightly worse result (accuracy: 82.01% vs. 85.99%) than the one with one class per website. We assume that this happens because of overﬁtting, 13 Fig. 11: Closed-world website classiﬁer: confusion matrices for different scenarios the classiﬁer becomes trimmed to detect single webpages it has already seen and not to generalize characteristics of thewebsite.

 To sum up, website classiﬁcation in a closed-world sce nario is signiﬁcantly more difﬁcult compared to index page classiﬁcation as it is typically performed in the related work. In reality, it is to be expected that for certain websites the adversary is not able to train on all sub-pages due to their number, similar to the case that he cannot train a webpage classiﬁer on the whole universe. We experimented reducing the number of pages available for training to 20 and tested against the remaining 30 pages. As expected, the accuracy degraded to 69.65%. None of our evaluated strategies improved the probability of success. However, the results do not indicate that website ﬁngerprinting is generally infeasible (since several websites, e.g., KICKASS or XNXX are reliably detectable inthis setting, see Figure 11).  

---

 ## 翻译[17/19]：
 

E. 网站检测

我们在前一节中已经证明指纹识别单个网页是不可行的。我们现在探讨另一种更现实的攻击场景，即对整个网站进行监控。在这种情况下，在上述网页场景中被描述为最优的策略将会是不利的，因为为网站的每个页面训练单独的分类器会显著增加期望的误报数（因为分类由多个决策组成，而单个误判就足以混淆两个网站）。一般来说，一个网页可能只会由于动态内容而改变。Juarez等人已经显示了页面更新对分类精度的深远影响。然而，网站还可能由于添加新页面或删除现有页面而发生变化。此外，对于许多网站来说，由于它们包含的数量巨大，例如Facebook.com，实际上不可能对每个页面进行指纹识别。因此，创建网站分类器是一个更具挑战性的目标。因此，我们现在分析攻击者可能遵循哪些攻击策略并分析它们的有效性。为了在这方面达到真实性，我们只允许攻击者使用可用页面的子集来训练网站。

首先，我们研究最简单的情况，以了解与网页分类器相关问题的复杂度。为此，我们进行了以下实验。在两个不相交的封闭世界设置中，我们旨在区分WEBSITES数据集中的20个网站。在情况(a)中，一个网站只由其索引页面的多个实例表示，因为通常是在相关工作中评估的。这对应着分类网页的问题。在情况(b)中，一个站点由其网页的子集给出。在两种情况下，我们使用每类（网站）51个实例，即在情况(a)中使用所有可用的索引页面实例，在情况(b)中使用51个不同的其他非索引页面之一的实例。图11显示了使用热图的两种情况的混淆矩阵。我们可以看到，基于其索引页面，网站可以被“完美地”分离（准确率为99%）。相反，基于网页子集的分类即使在这种微小的封闭世界设置中也不太准确（准确率为82%）。我们现在考虑攻击者可能采取的不同攻击策略以提高网站分类准确性。第一种策略假设索引页面特别适用于整个网站。因此，在情况(b)中的分类器的训练集中包括了它（共20个实例）。然而，这个假设被证明是错误的：在训练期间，索引页面的存在或缺失并不影响结果的准确性。第二个策略是只有一个非索引网页的实例可能是不够的，因此可能会降低结果的准确性。然而，即使每个网站的50个网页中使用了每个15个实例，也没有显著提高分类的准确性（准确率为85.99%）。第三种策略不仅考虑每个网站一个类别，而是分别对站点中的每个页面进行分类，以考虑它们的差异性。然后，在表示同一网站的不同网页的类中的混淆被忽略并视为真正的正样本。根据这种策略构建的分类器甚至产生了一个稍微更糟的结果（准确率为82.01%与85.99%相比）。我们假设发生这种情况是因为过度配合，分类器变得经过修剪，仅能检测它已经看到的单个网页，而不能推广网站的特征。

总之，在封闭世界情境中的网站分类比通常在相关工作中执行的索引页面分类更加困难。在现实中，可以预期对于某些网站，攻击者由于其数量无法训练所有子页面，类似于他不能在整个宇宙中训练网页分类器的情况。我们试验将可用于训练的页面数量减少到20，并针对其余30个页面进行测试。如预期的那样，准确性下降到69.65%。我们评估的策略没有一个能提高成功的概率。然而，结果并不表示网站指纹识别通常是不可能的（因为在这种情况下，几个网站，例如KICKASS或XNXX是可靠的可检测的，请参见图11）。

## 

---

 ## 原文[18/19]： 

 
 Moreover, the transition of results obtained in closed world to the realistic open-world setting is typically not trivial. We evaluated website ﬁngerprinting in the open world, using the WEBSITES dataset as the foreground and RND-WWW as the background. Figure 12 shows the average precision and recall (together with 95% conﬁdence intervals) for increasing background set sizes. For comparison, we derived the same visualization also for the (unﬁltered) webpage ﬁngerprinting scenario (i.e., from the data shown in Figures 7b and 8b). As the results indicate, website ﬁngerprinting scales better than web page ﬁngerprinting, although also for this objective precision and recall decline for increasing background set sizes. However, it appears that the precision stabilizes on a high level in the case of website classiﬁcation. Although the results obtained in closed-world settings suggested that website classiﬁcation is less promising than webpage classiﬁcation (here represented by index-page classiﬁcation), the results in the open world reveal the contrary. This also substantiates our assumption that closed-world results cannot be trivially generalized. In summary, to optimize his probability to succeed Fig. 12: RND-WWW: precision and recall for increasing back ground set sizes, classiﬁcation of websites (left-hand) vs. web pages (right-hand) in website ﬁngerprinting, the attacker should crawl many different pages of a site in favor of crawling many instances per page or of overestimating the importance of the index page.VIII.

 In this paper we proposed a novel website ﬁngerprinting approach, which we showed to be superior both in terms of detection accuracy and computational efﬁciency. For a com prehensive evaluation of attacks in this domain we compiled the most representative large-scale datasets of webpages that are actually retrieved on the Internet known to date. This allows for the ﬁrst time the evaluation of the WFP attack against Tor using realistic background noise. By publishing our datasets and tools we want to advance the ongoing research and discussion in this important ﬁeld. We are the ﬁrst to evaluate the WFP attack at Internet scale by avoiding the simpliﬁcation made in the related work that particularly background trafﬁc only consists of the transmission of index pages of popular websites. As we showed, webpage ﬁngerprinting does not scale for any considered page in our datasets and any state-of-the-art classiﬁer. Hence, the attack cannot be reliably used to convict users, but it may be used to limit the set of possible suspects. 14 The attack scenario to ﬁngerprint websites, i.e., a collection of webpages served under the same domain, is not only more realistic but also signiﬁcantly more effective using our attack method. We investigated several strategies to improve the success probability and emphasized the most promising tactics. Using our realistic datasets, a fact to be considered in future work is that users often remain on a website, i.e., they retrieve multiple pages of that site consecutively, e.g., by following links. We assume, that exploiting this information can furtherincrease the adversary’s conﬁdence.

 The authors would like to thank Norbert Landa and Robert Echelmeyer for their support while performing some of the experiments for this paper. Parts of this work have been funded by the EU H2020 Project “Privacy Flag”, the Luxembourg National Research Fund (FNR), and the Excellence Initiative of the German federal and state governments. Listing 1: Sites included in the WEBSITES dataset 15  

---

 ## 翻译[18/19]：
 

另外，将封闭世界中得出的结果转换为真实的开放世界设置通常并不容易。我们使用WEBSITES数据集作为前景和RND-WWW作为背景，在开放世界中评估了网站指纹识别。图12显示了随着背景集大小的增加的平均精度和召回率（连同95％置信区间）。为了比较，我们还从图7b和8b中显示的（未经过滤的）网页指纹识别场景中提取了相同的可视化效果。结果表明，网站指纹识别比网页指纹识别更具可扩展性，尽管对于这个目标，随着背景集大小的增加，精度和召回率也会下降。然而，对于网站分类，它似乎准确率在高水平上稳定。虽然在封闭世界设置中获得的结果表明，网站分类比网页分类不太有前途（这里由指数页分类表示），但开放世界中的结果却相反。这也证实了我们的假设，即封闭世界结果不能轻易地推广。总之，在网站指纹识别中，为了优化攻击者成功的概率，攻击者应该爬取站点的许多不同页面，而不是每页爬取许多实例或高估索引页面的重要性。 

在本文中，我们提出了一种新的网站指纹识别方法，我们证明它在检测准确性和计算效率方面都优于其他方法。为了全面评估此领域中的攻击，我们编制了迄今为止已知的具有代表性的大规模网络页面数据集。这一点首次允许对使用真实背景噪声的Tor的WFP攻击进行评估。通过发布我们的数据集和工具，我们希望推进这一重要领域的持续研究和讨论。我们是第一个通过避免与关联工作中的简化，即特定背景流量仅包括流行网站的索引页传输，而在互联网规模上评估WFP攻击的人。正如我们所示，无论在我们的数据集中考虑的任何页面和任何最先进的分类器上，网页指纹识别都不具有可扩展性。因此，攻击不能可靠地用于判定用户，但可以用于限制可能嫌疑人的范围14 。识别网站的攻击场景，即在同一域名下服务的一组网页，不仅更为现实，而且使用我们的攻击方法明显更有效。我们研究了几种策略以提高成功概率，并强调了最有前途的战术。使用我们的真实数据集，未来要考虑的一个事实是，用户经常停留在一个网站上，即他们连续检索该站点的多个页面，例如通过链接。我们认为，利用这些信息可以进一步增强攻击者的信心。

作者们要感谢Norbert Landa和Robert Echelmeyer在本文中进行一些实验时的支持。本工作的部分经费来自欧盟H2020项目“隐私旗帜”、卢森堡国家研究基金（FNR）和德国联邦和州政府的卓越计划。清单1：包含在WEBSITES数据集中的网站15。

